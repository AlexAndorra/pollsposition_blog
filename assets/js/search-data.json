{
  
    
        "post0": {
            "title": "Popularity hide and seek",
            "content": "A few months ago, I experimented with a Gaussian Process to estimate the popularity of French presidents across time. The experiment was really positive, and helped me get familiar with the beauty of GPs. This time, I teamed up with R√©mi Louf on a Markov Chain model to estimate the same process -- what is the true latent popularity, that we only observe through the noisy data that are polls? . This was supposed to be a trial run before working on an electoral model for the coming regional elections in France -- it&#39;s always easier to start with 2 dimensions than 6, right? But the model turned out to be so good at smoothing and predicting popularity data that we thought it&#39;d be a shame not to share it. And voil√†! . Show me the data! . The data are the same as in my GP post, so we&#39;re not going to spend a lot of time explaining them. It&#39;s basically all the popularity opinion polls of French presidents since the term limits switched to 5 years (in 2002). . Let&#39;s import those data, as well as the (fabulous) packages we&#39;ll need: . import datetime import arviz import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc3 as pm import theano.tensor as aet from scipy.special import expit as logistic . data = pd.read_csv( &quot;https://raw.githubusercontent.com/AlexAndorra/pollsposition_models/master/data/raw_popularity_presidents.csv&quot;, header=0, index_col=0, parse_dates=True, ) . The number of polls is homogeneous among months, except in the summer because, well, France: . data[&quot;month&quot;].value_counts().sort_index() . 1 100 2 96 3 100 4 89 5 91 6 95 7 68 8 71 9 94 10 99 11 98 12 82 Name: month, dtype: int64 . Let us look at simple stats on the pollsters: . pd.crosstab(data.sondage, data.method, margins=True) . method face to face internet phone phone&amp;internet All . sondage . BVA 0 | 101 | 89 | 0 | 190 | . Elabe 0 | 52 | 0 | 0 | 52 | . Harris 0 | 33 | 0 | 0 | 33 | . Ifop 0 | 29 | 181 | 38 | 248 | . Ipsos 0 | 40 | 177 | 0 | 217 | . Kantar 208 | 4 | 0 | 0 | 212 | . Odoxa 0 | 67 | 0 | 0 | 67 | . OpinionWay 0 | 12 | 0 | 0 | 12 | . Viavoice 0 | 20 | 0 | 0 | 20 | . YouGov 0 | 32 | 0 | 0 | 32 | . All 208 | 390 | 447 | 38 | 1083 | . Interesting: most pollsters only use one method -- internet. Only BVA, Ifop, Ipsos (and Kantar very recently) use different methods. So, if we naively estimate the biases of pollsters and methods individually, we&#39;ll get high correlations in our posterior estimates -- the parameter for face to face will basically be the one for Kantar, and vice versa. So we will need to model the pairs (pollster, method) rather than pollsters and methods individually. . Now, let&#39;s just plot the raw data and see what they look like: . approval_rates = data[&quot;p_approve&quot;].values disapproval_rates = data[&quot;p_disapprove&quot;].values doesnotrespond = 1 - approval_rates - disapproval_rates newterm_dates = data.reset_index().groupby(&quot;president&quot;).first()[&quot;index&quot;].values dates = data.index fig, axes = plt.subplots(2, figsize=(12, 6)) for ax, rate, label in zip( axes.ravel(), [approval_rates, doesnotrespond], [&quot;Approve&quot;, &quot;No answer&quot;], ): ax.plot(dates, rate, &quot;o&quot;, alpha=0.4) ax.set_ylim(0, 1) ax.set_ylabel(label) for date in newterm_dates: ax.axvline(date, color=&quot;k&quot;, alpha=0.6, linestyle=&quot;--&quot;) . We notice two things when looking at these plots: . Approval rates systematically decrease as the goes on. | While that&#39;s true, some events seem to push the approval rate back up, even though temporarily. This happened in every term, actually. Can that variance really be explained solely with a random walk? | Non-response rate is higher during Macron&#39;s term. | Monthly standard deviation . Something that often proves challenging with count data is that they are often more dispersed than traditional models expect them to be. Let&#39;s check this now, by computing the monthly standard deviation of the approval rates (we weigh each poll equally, even though we probably should weigh them according to their respective sample size): . rolling_std = ( data.reset_index() .groupby([&quot;year&quot;, &quot;month&quot;]) .std() .reset_index()[[&quot;year&quot;, &quot;month&quot;, &quot;p_approve&quot;]] ) rolling_std . year month p_approve . 0 2002 | 5 | 0.017078 | . 1 2002 | 6 | 0.030000 | . 2 2002 | 7 | 0.005774 | . 3 2002 | 8 | 0.045826 | . 4 2002 | 9 | 0.025166 | . ... ... | ... | ... | . 223 2020 | 12 | 0.064627 | . 224 2021 | 1 | 0.042661 | . 225 2021 | 2 | 0.041748 | . 226 2021 | 3 | 0.042980 | . 227 2021 | 4 | 0.020000 | . 228 rows √ó 3 columns . fig, ax = plt.subplots(figsize=(10, 4)) ax.plot( pd.to_datetime( [f&quot;{y}-{m}-01&quot; for y, m in zip(rolling_std.year, rolling_std.month)] ), rolling_std.p_approve.values, &quot;o&quot;, alpha=0.5, ) ax.set_title(&quot;Monthly standard deviation in polls&quot;) for date in newterm_dates: ax.axvline(date, color=&quot;k&quot;, alpha=0.6, linestyle=&quot;--&quot;) . There is a very high variance for Chirac&#39;s second term, and for the beggining of Macron&#39;s term. For Chirac&#39;s term, it seems like the difference stems from the polling method: face-to-face approval rates seem to be much lower, as you can see in the figure below. For Macron, this high variance is quite hard to explain. In any case, we&#39;ll probably have to take this overdispersion (as it&#39;s called in statistical linguo) of the data in our models... . face = data[data[&quot;method&quot;] == &quot;face to face&quot;] dates_face = face.index other = data[data[&quot;method&quot;] != &quot;face to face&quot;] dates_other = other.index fig, ax = plt.subplots(figsize=(10, 4)) ax.plot(dates_face, face[&quot;p_approve&quot;].values, &quot;o&quot;, alpha=0.3, label=&quot;face to face&quot;) ax.plot(dates_other, other[&quot;p_approve&quot;].values, &quot;o&quot;, alpha=0.3, label=&quot;other&quot;) ax.set_ylim(0, 1) ax.set_ylabel(&quot;Does approve&quot;) ax.set_title(&quot;Raw approval polls&quot;) ax.legend() for date in newterm_dates: ax.axvline(date, color=&quot;k&quot;, alpha=0.6, linestyle=&quot;--&quot;) . A raw analysis of bias . As each pollster uses different methods to establish and question their samples each month, we don&#39;t expect their results to be identical -- that would be troubling. Instead we expect each pollster and each polling method to be at a different place on the spectrum: some report popularity rates in line with the market average, some are below average, some are above. . The model will be able to estimate this bias on the fly and more seriously (if we tell it to), but let&#39;s take a look at a crude estimation ourselves, to get a first idea. Note that we&#39;re talking about statistical bias here, not political bias: it&#39;s very probable that reaching out to people only by internet or phone can have a selection effect on your sample, without it being politically motivated -- statistics are just hard and stubborn you know ü§∑‚Äç‚ôÇÔ∏è . To investigate bias, we now compute the monthly mean of the $p_{approve}$ values and check how each individual poll strayed from this mean: . data = ( data.reset_index() .merge( data.groupby([&quot;year&quot;, &quot;month&quot;])[&quot;p_approve&quot;].mean().reset_index(), on=[&quot;year&quot;, &quot;month&quot;], suffixes=[&quot;&quot;, &quot;_mean&quot;], ) .rename(columns={&quot;index&quot;: &quot;field_date&quot;}) ) data[&quot;diff_approval&quot;] = data[&quot;p_approve&quot;] - data[&quot;p_approve_mean&quot;] data.round(2) . field_date president sondage samplesize method p_approve p_disapprove year month p_approve_mean diff_approval . 0 2002-05-15 | chirac2 | Ifop | 924 | phone | 0.51 | 0.44 | 2002 | 5 | 0.50 | 0.01 | . 1 2002-05-20 | chirac2 | Kantar | 972 | face to face | 0.50 | 0.48 | 2002 | 5 | 0.50 | -0.00 | . 2 2002-05-23 | chirac2 | BVA | 1054 | phone | 0.52 | 0.37 | 2002 | 5 | 0.50 | 0.02 | . 3 2002-05-26 | chirac2 | Ipsos | 907 | phone | 0.48 | 0.48 | 2002 | 5 | 0.50 | -0.02 | . 4 2002-06-16 | chirac2 | Ifop | 974 | phone | 0.49 | 0.43 | 2002 | 6 | 0.50 | -0.02 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1078 2021-03-29 | macron | Kantar | 1000 | internet | 0.36 | 0.58 | 2021 | 3 | 0.38 | -0.02 | . 1079 2021-03-30 | macron | YouGov | 1068 | internet | 0.30 | 0.61 | 2021 | 3 | 0.38 | -0.08 | . 1080 2021-04-07 | macron | Elabe | 1003 | internet | 0.33 | 0.63 | 2021 | 4 | 0.35 | -0.02 | . 1081 2021-04-10 | macron | Ipsos | 1002 | internet | 0.37 | 0.58 | 2021 | 4 | 0.35 | 0.02 | . 1082 2021-04-26 | macron | Kantar | 1000 | internet | 0.35 | 0.58 | 2021 | 4 | 0.35 | 0.00 | . 1083 rows √ó 11 columns . Then, we can aggregate these offsets by pollster and look at their distributions: . POLLSTER_VALS = { pollster: data[data[&quot;sondage&quot;] == pollster][&quot;diff_approval&quot;].values for pollster in list(POLLSTERS) } colors = plt.rcParams[&quot;axes.prop_cycle&quot;]() fig, axes = plt.subplots(ncols=2, nrows=5, sharex=True, figsize=(12, 12)) for ax, (pollster, vals) in zip(axes.ravel(), POLLSTER_VALS.items()): c = next(colors)[&quot;color&quot;] ax.hist(vals, alpha=0.3, color=c, label=pollster) ax.axvline(x=np.mean(vals), color=c, linestyle=&quot;--&quot;) ax.axvline(x=0, color=&quot;black&quot;) ax.set_xlim(-0.3, 0.3) ax.legend() plt.xlabel(r&quot;$p_{approve} - bar{p}_{approve}$&quot;, fontsize=25); . A positive (resp. negative) bias means the pollster tends to report higher (resp. lower) popularity rates than the average pollster. We&#39;ll see what the model has to say about this, but our prior is that, for instance, YouGov and Kantar tend to be below average, while Harris and BVA tend to be higher. . And now for the bias per method: . METHOD_VALS = { method: data[data[&quot;method&quot;] == method][&quot;diff_approval&quot;].values for method in list(data[&quot;method&quot;].unique()) } colors = plt.rcParams[&quot;axes.prop_cycle&quot;]() fig, ax = plt.subplots(figsize=(11, 5)) for method, vals in METHOD_VALS.items(): c = next(colors)[&quot;color&quot;] ax.hist(vals, alpha=0.3, color=c, label=method) ax.axvline(x=np.mean(vals), color=c, linestyle=&quot;--&quot;) ax.axvline(x=0, color=&quot;black&quot;) ax.set_xlim(-0.2, 0.2) ax.set_xlabel(r&quot;$p_+ - bar{p}_{+}$&quot;, fontsize=25) ax.legend(); . Face-to-face polls seem to give systematically below-average approval rates, while telephone polls seem to give slightly higher-than-average results. . Again, keep in mind that there is substantial correlation between pollsters and method, so take this with a grain of salt -- that&#39;s why it&#39;s useful to add that to the model actually: it will be able to decipher these correlations, integrate them into the full data generating process, and report finer estimates of each bias. . Speaking of models, do you know what time it is? It&#39;s model time, of course!! . Specifying the model . We&#39;ll build several versions of our model, refining it incrementally. But the basic structure will remain the same. Let&#39;s build an abstract version that will help you understand the code. . Each poll $i$ at month $m$ from the beginning of a president‚Äôs term finds that $y_i$ individuals have a positive opinion of the president‚Äôs action over $n_i$ respondents. We model this as: . $$y_{i,m} sim Binomial(p_{i,m}, n_{i,m})$$ . We loosely call $p_{i,m}$ the popularity of the president in poll $i$, $m$ months into his presidency. . Why specify the month when the time information is already contained in the succession of polls? Because French people tend to be less and less satisfied with their president as their term moves, regardless of their action -- you&#39;ll see... . We model $p_{i,m}$ with a random walk logistic regression: . $$p_{i,m} = logistic( mu_m + alpha_k + zeta_j)$$ . $ mu_m$ is the latent support for the president at month $m$ and it&#39;s the main quantity we would like to model. $ alpha_k$ is the bias of the pollster, while $ zeta_j$ is the inherent bias of the polling method. The biases are assumed to be completely unpooled at first, i.e we model one bias for each pollster and method: . $$ alpha_k sim Normal(0, sigma_k) qquad forall , pollster , k$$ . and . $$ zeta_j sim Normal(0, sigma_j) qquad forall , method , j$$ . We treat the time variation of $ mu$ with a correlated random walk: . $$ mu_m | mu_{m-1} sim Normal( mu_{m-1}, sigma_m)$$ . Again, note that $ mu$ is latent: we never get to observe it in the world. . For the sake of simplicity, we choose not to account at first for a natural decline in popularity $ delta$, the unmeployment at month $m$, or random events that can happen during the term. . Mark What ? . Thus defined, our model is a Markov Model. This is a big and scary word to describe what is actually a simple concept (tip: this is a common technique to make us statisticians look cool and knowledgeable): a model where a &quot;hidden&quot; state jumps from one time step to another and where the observations are a function of this hidden state. Hidden states have no memory, in the sense that their value at any time step only depends on the value of the state at the previous time step. That&#39;s what markovian means. . Here, the hidden state is the latent popularity $ mu_m$ and we combine it with the effects $ alpha_k$ and $ zeta_j$ to compute the value of the observed states, the polling results $y_{i,m}$. The value of the latent popularity at month $m$ only depends on its value at $m-1$, and the jumps between months are normally distributed. . To define our model, we&#39;ll use PyMC&#39;s named coordinates feature. That way, we&#39;ll be able to write down our model using the names of variables instead of their shape dimensions. To do that, we need to define a bunch of variables: . pollster_by_method_id, pollster_by_methods = data.set_index( [&quot;sondage&quot;, &quot;method&quot;] ).index.factorize(sort=True) month_id = np.hstack( [ pd.Categorical( data[data.president == president].field_date.dt.to_period(&quot;M&quot;) ).codes for president in data.president.unique() ] ) months = np.arange(max(month_id) + 1) data[&quot;month_id&quot;] = month_id . COORDS = { &quot;pollster_by_method&quot;: pollster_by_methods, &quot;month&quot;: months, # each observation is uniquely identified by (pollster, field_date): &quot;observation&quot;: data.set_index([&quot;sondage&quot;, &quot;field_date&quot;]).index, } . Fixed sigma for the random walk . Our first model is as simple as possible: just a random walk on the monthly latent popularity and a term for the bias of each (pollster, method) pair, which is called the &quot;house effect&quot; in the political science litterature. Also, we&#39;ll use a more descriptive name for $ mu$ -- month_effect sounds good, because, well, that&#39;s basically what it is. We&#39;ll arbitrarily fix the innovation of the random walk (sigma) to 1 and see how it fares. . with pm.Model(coords=COORDS) as pooled_popularity_simple: house_effect = pm.Normal(&quot;house_effect&quot;, 0, 0.15, dims=&quot;pollster_by_method&quot;) month_effect = pm.GaussianRandomWalk(&quot;month_effect&quot;, sigma=1.0, dims=&quot;month&quot;) popularity = pm.math.invlogit( month_effect[month_id] + house_effect[pollster_by_method_id] ) N_approve = pm.Binomial( &quot;N_approve&quot;, p=popularity, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [month_effect, house_effect] . . 100.00% [8000/8000 00:23&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 42 seconds. . 0, dim: observation, 1083 =? 1083 . The acceptance probability does not match the target. It is 0.3442984443078715, but should be close to 0.8. Try to increase the number of tuning steps. The rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling. The estimated number of effective samples is smaller than 200 for some parameters. . We plot the posterior distribution of the pollster and method biases: . arviz.plot_trace(idata); . Because of the logistic link function, these coefficients can be tricky to interpret. When the bias is positive, this means that we need to add to the latent popularity to get the observation, which means that the (pollster, method) pair tends to be biased towards giving higher popularity scores. . This model clearly has issues: the trace plot is really ugly and the R-hat statistic is larger than 1.2 for some parameters, which indicates problems during sampling. This is not surprising: this model is really simple. The important thing here is to diagnose the depth of the pathologies, and see how that points us to improvements. . Let&#39;s look at the summary table: . arviz.summary(idata, round_to=2) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . house_effect[0] -0.10 | 0.04 | -0.19 | -0.03 | 0.01 | 0.01 | 23.23 | 59.59 | 1.15 | . house_effect[1] 0.38 | 0.04 | 0.30 | 0.45 | 0.01 | 0.01 | 22.39 | 62.47 | 1.15 | . house_effect[2] -0.15 | 0.04 | -0.23 | -0.08 | 0.01 | 0.01 | 22.52 | 64.04 | 1.15 | . house_effect[3] 0.34 | 0.04 | 0.26 | 0.42 | 0.01 | 0.01 | 23.70 | 63.04 | 1.14 | . house_effect[4] 0.05 | 0.04 | -0.03 | 0.13 | 0.01 | 0.01 | 24.94 | 65.23 | 1.14 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . month_effect[55] -0.93 | 0.04 | -1.02 | -0.85 | 0.01 | 0.01 | 30.41 | 93.90 | 1.11 | . month_effect[56] -0.96 | 0.04 | -1.04 | -0.87 | 0.01 | 0.01 | 26.41 | 85.65 | 1.12 | . month_effect[57] -0.94 | 0.04 | -1.02 | -0.85 | 0.01 | 0.01 | 25.97 | 81.38 | 1.12 | . month_effect[58] -0.78 | 0.04 | -0.86 | -0.70 | 0.01 | 0.01 | 25.49 | 90.16 | 1.13 | . month_effect[59] -0.75 | 0.04 | -0.83 | -0.66 | 0.01 | 0.01 | 25.33 | 86.35 | 1.13 | . 75 rows √ó 9 columns . Wow, that&#39;s bad! Do you see these much-too-high R_hat and much-too-low effective sample sizes (ess_bulk and ess_tail)? . Let&#39;s not spend too much time on this model, but before we move on, it&#39;s useful to see how bad our posterior predictions for mu, the estimated monthly latent popularity, look. Since the model is completely pooled, we only have 60 values, which correspond to a full term (i.e 5 years): . def plot_latent_mu(inference_data, overlay_observed=True): &quot;&quot;&quot;Plot latent posterior popularity&quot;&quot;&quot; post_pop = logistic( inference_data.posterior[&quot;month_effect&quot;].stack(sample=(&quot;chain&quot;, &quot;draw&quot;)) ) fig, ax = plt.subplots() # plot random posterior draws ax.plot( inference_data.posterior.coords[&quot;month&quot;], post_pop.isel( sample=np.random.choice(post_pop.coords[&quot;sample&quot;].size, size=1000) ), alpha=0.01, color=&quot;grey&quot;, ) # plot posterior mean post_pop.mean(&quot;sample&quot;).plot(ax=ax, color=&quot;black&quot;, lw=2, label=&quot;predicted mean&quot;) # plot monthly raw polls if overlay_observed: obs_mean = ( data.groupby([&quot;president&quot;, &quot;month_id&quot;]).last()[&quot;p_approve_mean&quot;].unstack().T ) for president in obs_mean.columns: ax.plot( obs_mean.index, obs_mean[president], &quot;o&quot;, alpha=0.3, label=f&quot;obs. monthly {president}&quot;, ) ax.set_xlabel(&quot;Months into term&quot;) ax.set_ylabel(&quot;Does approve&quot;) ax.legend() . plot_latent_mu(idata) . Not too good, is it? The black line is the mean posterior latent monthly popularity estimated by the model. Each grey line is a draw from the posterior latent popularity, and each point is the observed monthly mean popularity in polls for each president. . No need to stare at this graph to notice that the model grossly underestimates the variance in the data. We also see that presidents differ quite a lot, although they have some common pattern (this is a clue for improving the model; can you guess how we could include that?). The good point though is that the model is highly influenced by the sample size: up until month 50, the posterior prediction stays close to wherever the most dots are clustered, because those values appear most frequently, so it&#39;s a safer bet. Between months 50 and 60, polls become more dispersed, so the model is doing a compromise, staying below the bulk of points but much higher than the lowest points. Here, what&#39;s troubling the model is that one of the presidents (Fran√ßois Hollande) was hugely unpopular at the end of his term compared to the others. . An easy and obvious way to improve this model is to allow the random walk&#39;s innovation to vary more. Maybe our model is too constrained by the fixed innovation and can&#39;t accomodate the variation in the data? . Infer the standard deviation of the random walk . Instead of fixing the random walk&#39;s innovation, let&#39;s estimate it from the data. The code is very similar: . with pm.Model(coords=COORDS) as pooled_popularity: house_effect = pm.Normal(&quot;house_effect&quot;, 0, 0.15, dims=&quot;pollster_by_method&quot;) sigma_mu = pm.HalfNormal(&quot;sigma_mu&quot;, 0.5) month_effect = pm.GaussianRandomWalk(&quot;month_effect&quot;, sigma=sigma_mu, dims=&quot;month&quot;) popularity = pm.math.invlogit( month_effect[month_id] + house_effect[pollster_by_method_id] ) N_approve = pm.Binomial( &quot;N_approve&quot;, p=popularity, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(tune=2000, draws=2000, return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [month_effect, sigma_mu, house_effect] . . 100.00% [16000/16000 00:29&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 47 seconds. . 0, dim: observation, 1083 =? 1083 . The acceptance probability does not match the target. It is 0.9081390089258286, but should be close to 0.8. Try to increase the number of tuning steps. The estimated number of effective samples is smaller than 200 for some parameters. . Did this help convergence? . arviz.plot_trace(idata); . Aaaaah, my eyes, my eyes, please stop! . These trace plots are still very ugly. What about the R-hats and effective sample sizes? . arviz.summary(idata, round_to=2) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . house_effect[0] -0.10 | 0.04 | -0.17 | -0.03 | 0.0 | 0.0 | 69.19 | 126.68 | 1.03 | . house_effect[1] 0.38 | 0.04 | 0.31 | 0.45 | 0.0 | 0.0 | 70.21 | 134.31 | 1.03 | . house_effect[2] -0.15 | 0.04 | -0.22 | -0.07 | 0.0 | 0.0 | 71.80 | 132.09 | 1.03 | . house_effect[3] 0.34 | 0.04 | 0.27 | 0.41 | 0.0 | 0.0 | 74.23 | 150.45 | 1.03 | . house_effect[4] 0.05 | 0.04 | -0.02 | 0.12 | 0.0 | 0.0 | 74.48 | 152.07 | 1.03 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . month_effect[56] -0.96 | 0.04 | -1.03 | -0.88 | 0.0 | 0.0 | 85.24 | 198.54 | 1.03 | . month_effect[57] -0.93 | 0.04 | -1.01 | -0.85 | 0.0 | 0.0 | 86.12 | 189.24 | 1.03 | . month_effect[58] -0.79 | 0.04 | -0.87 | -0.71 | 0.0 | 0.0 | 85.96 | 177.54 | 1.03 | . month_effect[59] -0.75 | 0.04 | -0.83 | -0.67 | 0.0 | 0.0 | 86.35 | 206.63 | 1.02 | . sigma_mu 0.10 | 0.01 | 0.08 | 0.12 | 0.0 | 0.0 | 4283.14 | 4889.75 | 1.00 | . 76 rows √ó 9 columns . Still very, very bad... The only good news is that we seem to efficiently estimate sigma_mu, the innovation of the random walk -- the R-hat is perfect and the ESS is high. . Do the posterior predictions look better? . plot_latent_mu(idata) . The posterior variance of the values of $ mu$ still is grossly underestimated; between month 40 and 50 presidents have had popularity rates between 0.2 and 0.4, while here the popularity is estimated to be around 0.21 plus or minus 0.02 at best. We need to fhix this. . Accounting for overdispersion in polls . As we saw with the previous model, the variance of $ mu$&#39;s posterior values is grossly underestimated. This comes from at least two things: . Presidents have similarities, but also a lot of differences in how their popularity rates evolves with time. We should take that into account and estimate one trendline per president. We&#39;ll do that later. . | Even beyond president effects, it seems that there is much more variation in the data than a Binomial distribution can account for (as is often the case with count data). This is called overdispersion of data in statistical linguo, and is due to the fact that the Binomial&#39;s variance depends on its mean. A convenient way to get around this limitation is to use a Beta-Binomial likelihood, to add one degree of freedom and allow the variance to be estimated independently from the mean value. For more details about this distribution and its parametrization, see this blog post. In short, this allows each poll to have its own Binomial probability, which even makes sense scientifically: it&#39;s conceivable that each poll is different in several ways from the others (even when done by the same pollster), because there are measurement errors and other factors we did not include, even beyond pollsters&#39; and method&#39;s biases. . | with pm.Model(coords=COORDS) as pooled_popularity: house_effect = pm.Normal(&quot;house_effect&quot;, 0, 0.15, dims=&quot;pollster_by_method&quot;) sigma_mu = pm.HalfNormal(&quot;sigma_mu&quot;, 0.5) month_effect = pm.GaussianRandomWalk(&quot;month_effect&quot;, sigma=sigma_mu, dims=&quot;month&quot;) popularity = pm.math.invlogit( month_effect[month_id] + house_effect[pollster_by_method_id] ) # overdispersion parameter theta = pm.Exponential(&quot;theta_offset&quot;, 1.0) + 10.0 N_approve = pm.BetaBinomial( &quot;N_approve&quot;, alpha=popularity * theta, beta=(1.0 - popularity) * theta, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(tune=2000, draws=2000, return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [theta_offset, month_effect, sigma_mu, house_effect] . . 100.00% [16000/16000 00:38&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 56 seconds. . 0, dim: observation, 1083 =? 1083 . The number of effective samples is smaller than 10% for some parameters. . arviz.plot_trace(idata); . arviz.summary(idata, round_to=2) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . house_effect[0] -0.09 | 0.06 | -0.20 | 0.01 | 0.00 | 0.00 | 970.36 | 2360.00 | 1.0 | . house_effect[1] 0.36 | 0.06 | 0.25 | 0.46 | 0.00 | 0.00 | 972.49 | 2209.77 | 1.0 | . house_effect[2] -0.11 | 0.06 | -0.23 | 0.02 | 0.00 | 0.00 | 1397.60 | 3077.42 | 1.0 | . house_effect[3] 0.30 | 0.07 | 0.16 | 0.44 | 0.00 | 0.00 | 1598.99 | 3445.40 | 1.0 | . house_effect[4] 0.06 | 0.07 | -0.09 | 0.19 | 0.00 | 0.00 | 1997.57 | 4200.72 | 1.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . month_effect[57] -0.90 | 0.08 | -1.05 | -0.75 | 0.00 | 0.00 | 1449.45 | 3172.23 | 1.0 | . month_effect[58] -0.85 | 0.08 | -1.00 | -0.69 | 0.00 | 0.00 | 1594.73 | 3531.40 | 1.0 | . month_effect[59] -0.83 | 0.09 | -1.00 | -0.65 | 0.00 | 0.00 | 2052.19 | 3857.49 | 1.0 | . sigma_mu 0.08 | 0.01 | 0.05 | 0.10 | 0.00 | 0.00 | 2309.07 | 3766.36 | 1.0 | . theta_offset 18.93 | 1.28 | 16.53 | 21.30 | 0.01 | 0.01 | 7712.70 | 5102.88 | 1.0 | . 77 rows √ó 9 columns . All of this is looking much better: only one sampling warning, really good-looking trace plot and much higher effective sample sizes (although it&#39;s still a bit low for some parameters). . What about the posterior predictions? . plot_latent_mu(idata) . This is better! We can see why the model is more comfortable: the Beta-Binomial likelihood give it more flexibility, as exemplified in the more wiggly posterior predictions, which also increases the uncertainty of the predictions. . Still, this is not very satisfactory. The main limit of this model is that it doesn&#39;t distinguish between presidents -- it pools all of them -- although they all have differences despite being similar in some ways. As a result, it is unlikely we would be able to do much better than this for the pooled model; maybe by having one dispersion term per term/month? . I don&#39;t know about you, but each time I hear &quot;similar but different&quot;, I immediately think of a hiearchical (i.e partially pooled) model (yeah, I&#39;m weird sometimes). Well, that&#39;s exactly what we&#39;re going to investigate next! . Respect the hierarchy . The main change is that now our month_effect will become a month_president_effect, and we&#39;ll have a common monthly mean for all presidents (which will be our new month_effect. A nice feature is that sigma_mu can now be interpreted as the shrinkage parameter of the random walk: the closest to zero it will be inferred to be, the more similar the presidents will be considered in their monthly popularity evolution. That&#39;s why we&#39;ll rename this parameter shrinkage_pop. Finally, the house effects stay unpooled, as they were before. . Let&#39;s code that up and sample! . president_id, presidents = data[&quot;president&quot;].factorize(sort=False) COORDS[&quot;president&quot;] = presidents . with pm.Model(coords=COORDS) as hierarchical_popularity: house_effect = pm.Normal(&quot;house_effect&quot;, 0, 0.15, dims=&quot;pollster_by_method&quot;) month_effect = pm.Normal(&quot;month_effect&quot;, 0, 0.15, shape=len(COORDS[&quot;month&quot;]) + 1) shrinkage_pop = pm.HalfNormal(&quot;shrinkage_pop&quot;, 0.2) month_president_effect = pm.GaussianRandomWalk( &quot;month_president_effect&quot;, mu=month_effect, sigma=shrinkage_pop, dims=(&quot;president&quot;, &quot;month&quot;), ) popularity = pm.math.invlogit( month_president_effect[president_id, month_id] + house_effect[pollster_by_method_id] ) N_approve = pm.Binomial( &quot;N_approve&quot;, p=popularity, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [month_president_effect, shrinkage_pop, month_effect, house_effect] . . 28.66% [2293/8000 01:53&lt;04:41 Sampling 4 chains, 0 divergences] RemoteTraceback Traceback (most recent call last) RemoteTraceback: &#34;&#34;&#34; Traceback (most recent call last): File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/parallel_sampling.py&#34;, line 137, in run self._start_loop() File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/parallel_sampling.py&#34;, line 191, in _start_loop point, stats = self._compute_point() File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/parallel_sampling.py&#34;, line 216, in _compute_point point, stats = self._step_method.step(self._point) File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/step_methods/arraystep.py&#34;, line 276, in step apoint, stats = self.astep(array) File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/step_methods/hmc/base_hmc.py&#34;, line 147, in astep self.potential.raise_ok(self._logp_dlogp_func._ordering.vmap) File &#34;/Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/step_methods/hmc/quadpotential.py&#34;, line 272, in raise_ok raise ValueError(&#34; n&#34;.join(errmsg)) ValueError: Mass matrix contains zeros on the diagonal. The derivative of RV `month_president_effect`.ravel()[1] is zero. The derivative of RV `month_president_effect`.ravel()[2] is zero. The derivative of RV `month_president_effect`.ravel()[4] is zero. The derivative of RV `month_president_effect`.ravel()[5] is zero. The derivative of RV `month_president_effect`.ravel()[10] is zero. The derivative of RV `month_president_effect`.ravel()[11] is zero. The derivative of RV `month_president_effect`.ravel()[12] is zero. The derivative of RV `month_president_effect`.ravel()[17] is zero. The derivative of RV `month_president_effect`.ravel()[18] is zero. The derivative of RV `month_president_effect`.ravel()[26] is zero. The derivative of RV `month_president_effect`.ravel()[38] is zero. The derivative of RV `month_president_effect`.ravel()[43] is zero. The derivative of RV `month_president_effect`.ravel()[47] is zero. The derivative of RV `month_president_effect`.ravel()[48] is zero. The derivative of RV `month_president_effect`.ravel()[50] is zero. The derivative of RV `month_president_effect`.ravel()[56] is zero. The derivative of RV `month_president_effect`.ravel()[61] is zero. The derivative of RV `month_president_effect`.ravel()[62] is zero. The derivative of RV `month_president_effect`.ravel()[64] is zero. The derivative of RV `month_president_effect`.ravel()[65] is zero. The derivative of RV `month_president_effect`.ravel()[68] is zero. The derivative of RV `month_president_effect`.ravel()[70] is zero. The derivative of RV `month_president_effect`.ravel()[71] is zero. The derivative of RV `month_president_effect`.ravel()[98] is zero. The derivative of RV `month_president_effect`.ravel()[101] is zero. The derivative of RV `month_president_effect`.ravel()[107] is zero. The derivative of RV `month_president_effect`.ravel()[108] is zero. The derivative of RV `month_president_effect`.ravel()[110] is zero. The derivative of RV `month_president_effect`.ravel()[119] is zero. The derivative of RV `month_president_effect`.ravel()[120] is zero. The derivative of RV `month_president_effect`.ravel()[121] is zero. The derivative of RV `month_president_effect`.ravel()[122] is zero. The derivative of RV `month_president_effect`.ravel()[125] is zero. The derivative of RV `month_president_effect`.ravel()[130] is zero. The derivative of RV `month_president_effect`.ravel()[138] is zero. The derivative of RV `month_president_effect`.ravel()[143] is zero. The derivative of RV `month_president_effect`.ravel()[146] is zero. The derivative of RV `month_president_effect`.ravel()[150] is zero. The derivative of RV `month_president_effect`.ravel()[167] is zero. The derivative of RV `month_president_effect`.ravel()[168] is zero. The derivative of RV `month_president_effect`.ravel()[170] is zero. The derivative of RV `month_president_effect`.ravel()[171] is zero. The derivative of RV `month_president_effect`.ravel()[179] is zero. The derivative of RV `month_president_effect`.ravel()[181] is zero. The derivative of RV `month_president_effect`.ravel()[197] is zero. The derivative of RV `month_president_effect`.ravel()[198] is zero. The derivative of RV `month_president_effect`.ravel()[200] is zero. The derivative of RV `month_president_effect`.ravel()[203] is zero. The derivative of RV `month_president_effect`.ravel()[206] is zero. The derivative of RV `month_president_effect`.ravel()[210] is zero. The derivative of RV `month_president_effect`.ravel()[211] is zero. The derivative of RV `month_president_effect`.ravel()[227] is zero. The derivative of RV `month_president_effect`.ravel()[230] is zero. The derivative of RV `month_president_effect`.ravel()[239] is zero. The derivative of RV `month_effect`.ravel()[11] is zero. The derivative of RV `month_effect`.ravel()[12] is zero. The derivative of RV `month_effect`.ravel()[13] is zero. The derivative of RV `month_effect`.ravel()[16] is zero. The derivative of RV `month_effect`.ravel()[17] is zero. The derivative of RV `month_effect`.ravel()[18] is zero. The derivative of RV `month_effect`.ravel()[19] is zero. The derivative of RV `month_effect`.ravel()[27] is zero. The derivative of RV `month_effect`.ravel()[28] is zero. The derivative of RV `month_effect`.ravel()[33] is zero. The derivative of RV `month_effect`.ravel()[51] is zero. The derivative of RV `month_effect`.ravel()[55] is zero. The derivative of RV `month_effect`.ravel()[60] is zero. &#34;&#34;&#34; The above exception was the direct cause of the following exception: ValueError Traceback (most recent call last) ValueError: Mass matrix contains zeros on the diagonal. The derivative of RV `month_president_effect`.ravel()[1] is zero. The derivative of RV `month_president_effect`.ravel()[2] is zero. The derivative of RV `month_president_effect`.ravel()[4] is zero. The derivative of RV `month_president_effect`.ravel()[5] is zero. The derivative of RV `month_president_effect`.ravel()[10] is zero. The derivative of RV `month_president_effect`.ravel()[11] is zero. The derivative of RV `month_president_effect`.ravel()[12] is zero. The derivative of RV `month_president_effect`.ravel()[17] is zero. The derivative of RV `month_president_effect`.ravel()[18] is zero. The derivative of RV `month_president_effect`.ravel()[26] is zero. The derivative of RV `month_president_effect`.ravel()[38] is zero. The derivative of RV `month_president_effect`.ravel()[43] is zero. The derivative of RV `month_president_effect`.ravel()[47] is zero. The derivative of RV `month_president_effect`.ravel()[48] is zero. The derivative of RV `month_president_effect`.ravel()[50] is zero. The derivative of RV `month_president_effect`.ravel()[56] is zero. The derivative of RV `month_president_effect`.ravel()[61] is zero. The derivative of RV `month_president_effect`.ravel()[62] is zero. The derivative of RV `month_president_effect`.ravel()[64] is zero. The derivative of RV `month_president_effect`.ravel()[65] is zero. The derivative of RV `month_president_effect`.ravel()[68] is zero. The derivative of RV `month_president_effect`.ravel()[70] is zero. The derivative of RV `month_president_effect`.ravel()[71] is zero. The derivative of RV `month_president_effect`.ravel()[98] is zero. The derivative of RV `month_president_effect`.ravel()[101] is zero. The derivative of RV `month_president_effect`.ravel()[107] is zero. The derivative of RV `month_president_effect`.ravel()[108] is zero. The derivative of RV `month_president_effect`.ravel()[110] is zero. The derivative of RV `month_president_effect`.ravel()[119] is zero. The derivative of RV `month_president_effect`.ravel()[120] is zero. The derivative of RV `month_president_effect`.ravel()[121] is zero. The derivative of RV `month_president_effect`.ravel()[122] is zero. The derivative of RV `month_president_effect`.ravel()[125] is zero. The derivative of RV `month_president_effect`.ravel()[130] is zero. The derivative of RV `month_president_effect`.ravel()[138] is zero. The derivative of RV `month_president_effect`.ravel()[143] is zero. The derivative of RV `month_president_effect`.ravel()[146] is zero. The derivative of RV `month_president_effect`.ravel()[150] is zero. The derivative of RV `month_president_effect`.ravel()[167] is zero. The derivative of RV `month_president_effect`.ravel()[168] is zero. The derivative of RV `month_president_effect`.ravel()[170] is zero. The derivative of RV `month_president_effect`.ravel()[171] is zero. The derivative of RV `month_president_effect`.ravel()[179] is zero. The derivative of RV `month_president_effect`.ravel()[181] is zero. The derivative of RV `month_president_effect`.ravel()[197] is zero. The derivative of RV `month_president_effect`.ravel()[198] is zero. The derivative of RV `month_president_effect`.ravel()[200] is zero. The derivative of RV `month_president_effect`.ravel()[203] is zero. The derivative of RV `month_president_effect`.ravel()[206] is zero. The derivative of RV `month_president_effect`.ravel()[210] is zero. The derivative of RV `month_president_effect`.ravel()[211] is zero. The derivative of RV `month_president_effect`.ravel()[227] is zero. The derivative of RV `month_president_effect`.ravel()[230] is zero. The derivative of RV `month_president_effect`.ravel()[239] is zero. The derivative of RV `month_effect`.ravel()[11] is zero. The derivative of RV `month_effect`.ravel()[12] is zero. The derivative of RV `month_effect`.ravel()[13] is zero. The derivative of RV `month_effect`.ravel()[16] is zero. The derivative of RV `month_effect`.ravel()[17] is zero. The derivative of RV `month_effect`.ravel()[18] is zero. The derivative of RV `month_effect`.ravel()[19] is zero. The derivative of RV `month_effect`.ravel()[27] is zero. The derivative of RV `month_effect`.ravel()[28] is zero. The derivative of RV `month_effect`.ravel()[33] is zero. The derivative of RV `month_effect`.ravel()[51] is zero. The derivative of RV `month_effect`.ravel()[55] is zero. The derivative of RV `month_effect`.ravel()[60] is zero. The above exception was the direct cause of the following exception: RuntimeError Traceback (most recent call last) &lt;ipython-input-31-4872755fffcc&gt; in &lt;module&gt; 24 ) 25 &gt; 26 idata = pm.sample(return_inferencedata=True) ~/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/sampling.py in sample(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, progressbar, model, random_seed, discard_tuned_samples, compute_convergence_checks, callback, jitter_max_retries, return_inferencedata, idata_kwargs, mp_ctx, pickle_backend, **kwargs) 557 _print_step_hierarchy(step) 558 try: --&gt; 559 trace = _mp_sample(**sample_args, **parallel_args) 560 except pickle.PickleError: 561 _log.warning(&#34;Could not pickle model, sampling singlethreaded.&#34;) ~/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/sampling.py in _mp_sample(draws, tune, step, chains, cores, chain, random_seed, start, progressbar, trace, model, callback, discard_tuned_samples, mp_ctx, pickle_backend, **kwargs) 1475 try: 1476 with sampler: -&gt; 1477 for draw in sampler: 1478 trace = traces[draw.chain - chain] 1479 if trace.supports_sampler_stats and draw.stats is not None: ~/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/parallel_sampling.py in __iter__(self) 477 478 while self._active: --&gt; 479 draw = ProcessAdapter.recv_draw(self._active) 480 proc, is_last, draw, tuning, stats, warns = draw 481 self._total_draws += 1 ~/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/parallel_sampling.py in recv_draw(processes, timeout) 357 else: 358 error = RuntimeError(&#34;Chain %s failed.&#34; % proc.chain) --&gt; 359 raise error from old_error 360 elif msg[0] == &#34;writing_done&#34;: 361 proc._readable = True RuntimeError: Chain 0 failed. . Uh-oh, our model doesn&#39;t sample... Apparently we&#39;ve got zero derivates for some variables, whatever that means! Usually, this is due to missing values somewhere (which leads to -infinity log-probabilities), or just to some misspecification in the model (yep, life is complicated, we&#39;ve got to accept it). A first step then is to check the model&#39;s test point and see whether we&#39;ve got any -inf in there: . hierarchical_popularity.check_test_point() . house_effect 14.67 month_effect 59.67 shrinkage_pop_log__ -0.77 month_president_effect 9895.94 N_approve -83867.83 Name: Log-probability of test_point, dtype: float64 . Nope, everything looks good. So, the the problem doesn&#39;t come from missing values in the data but certainly from the model specification itself. We&#39;ve checked, and there is no typo in the code above. A safe bet here is that the current parametrization (very poorly called &quot;centered&quot; parametrization) is somehow presenting the MCMC sampler with a vexing geometry. A common trick is to switch to a &quot;non-centered parametrization&quot;, where month_effect and shrinkage_pop are estimated independently from month_president_effect, as you&#39;ll see in the code below. . This trick is a bit weird if that&#39;s the first time you&#39;re encountering it, so you can take a look at this blog post for further explanation. . with pm.Model(coords=COORDS) as hierarchical_popularity: house_effect = pm.Normal(&quot;house_effect&quot;, 0, 0.15, dims=&quot;pollster_by_method&quot;) month_effect = pm.Normal(&quot;month_effect&quot;, 0, 0.15, dims=&quot;month&quot;) sd = pm.HalfNormal(&quot;shrinkage_pop&quot;, 0.2) raw_rw = pm.GaussianRandomWalk(&quot;raw_rw&quot;, sigma=1.0, dims=(&quot;president&quot;, &quot;month&quot;)) month_president_effect = pm.Deterministic( &quot;month_president_effect&quot;, month_effect + raw_rw * sd, dims=(&quot;president&quot;, &quot;month&quot;), ) popularity = pm.math.invlogit( month_president_effect[president_id, month_id] + house_effect[pollster_by_method_id] ) N_approve = pm.Binomial( &quot;N_approve&quot;, p=popularity, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [raw_rw, shrinkage_pop, month_effect, house_effect] . . 100.00% [8000/8000 01:58&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 136 seconds. . 0, dim: observation, 1083 =? 1083 . The number of effective samples is smaller than 25% for some parameters. . Yep, that was it! Feels like magic, doesn&#39;t it? Each time I just switch from a centered to a non-centered parametrization and it just starts sampling, I am amazed! . We only got a small warning about effective sample size, so we expect the trace plot to look good. But do our estimates make sense? . arviz.plot_trace( idata, ); . That looks a bit weird right? shrinkage_pop, the random walk&#39;s standard deviation, seems really high! That&#39;s basically telling us that the president&#39;s popularity can change a lot from one month to another, which we now from domain knowledge is not true. The month_effect are all similar and centered on 0, which means all months are very similar -- there can&#39;t really be a bad month or a good month. . This is worrying for at least two reasons: 1) we know from prior knowledge that there are good and bad months for presidents; 2) this extreme similarity in month_effect directly contradicts the high shrinkage_pop: how can the standard deviation be so high if months are all the same? . So something is missing here. Actually, we should really have an intercept, which represents the baseline presidential approval, no matter the month and president. The tricky thing here is that pm.GaussianRandomWalk uses a distribution to initiate the random walk. So, if we don&#39;t constrain it to zero, we will get an additive non-identifiability -- for each president and month, we&#39;ll have two intercepts, baseline and the initial value of the random walk. pm.GaussianRandomWalk only accepts distribution objects for the init kwarg though, so we have to implement the random walk by hand, i.e: . $$ mu_n = mu_{n - 1} + Z_n, , with , Z_n sim Normal(0, 1) , and , mu_0 = 0$$ . In other words, a Gaussian random walk is just a cumulative sum, where we add a sample from a standard Normal at each step ($Z_n$ here, which is called the innovation of the random walk). . Finally, it&#39;s probably useful to add a president_effect: it&#39;s very probable that some presidents are just more popular than others, even when taking into account the cyclical temporal variations. . COORDS[&quot;month_minus_origin&quot;] = COORDS[&quot;month&quot;][1:] . with pm.Model(coords=COORDS) as hierarchical_popularity: baseline = pm.Normal(&quot;baseline&quot;) president_effect = pm.Normal(&quot;president_effect&quot;, sigma=0.15, dims=&quot;president&quot;) house_effect = pm.Normal(&quot;house_effect&quot;, 0.15, dims=&quot;pollster_by_method&quot;) month_effect = pm.Normal(&quot;month_effect&quot;, 0.15, dims=&quot;month&quot;) # need the cumsum parametrization to properly control the init of the GRW rw_init = aet.zeros(shape=(len(COORDS[&quot;president&quot;]), 1)) rw_innovations = pm.Normal( &quot;rw_innovations&quot;, dims=(&quot;president&quot;, &quot;month_minus_origin&quot;), ) raw_rw = aet.cumsum(aet.concatenate([rw_init, rw_innovations], axis=-1), axis=-1) sd = pm.HalfNormal(&quot;shrinkage_pop&quot;, 0.2) month_president_effect = pm.Deterministic( &quot;month_president_effect&quot;, raw_rw * sd, dims=(&quot;president&quot;, &quot;month&quot;) ) popularity = pm.math.invlogit( baseline + president_effect[president_id] + month_effect[month_id] + month_president_effect[president_id, month_id] + house_effect[pollster_by_method_id] ) N_approve = pm.Binomial( &quot;N_approve&quot;, p=popularity, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) idata = pm.sample(return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [shrinkage_pop, rw_innovations, month_effect, house_effect, president_effect, baseline] . . 100.00% [8000/8000 11:50&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 733 seconds. . 0, dim: observation, 1083 =? 1083 . No warnings whatsoever! Who would have thought that adding a simple intercept would help that much! Let&#39;s look at our expectedly beautiful trace plot ü§© . Note that sampling time went up though -- interesting ü§î . arviz.plot_trace( idata, var_names=&quot;~rw&quot;, filter_vars=&quot;regex&quot;, ); . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 . That looks much better, doesn&#39;t it? Now we do see a difference in the different months, and the shrinkage standard deviation looks much more reasonable too, meaning that once we&#39;ve accounted for the variation in popularity associated with the other effects, the different presidents&#39; popularity isn&#39;t that different on a monthly basis -- i.e there are cycles in popularity, no matter who the president is. . Modelers just wanna have fuuuun! . We could stop there, but, for fun, let&#39;s improve this model even further by: . Using a Beta-Binomial likelihood. We already saw in the completely pooled model that it improves fit and convergence a lot. Plus, it makes scientific sense: for a lot of reasons, each poll probably has a different true Binomial probability than all the other ones -- even when it comes from the same pollster; just think about measurement errors or the way the sample is different each time. Here, we parametrize the Beta-Binomial by its mean and precision, instead of the classical $ alpha$ and $ beta$ parameters. For more details about this distribution and parametrization, see this blog post. . | Making sure that our different effects sum to zero. Think about the month effect. It only makes sense in a relative sense: some months are better than average, some others are worse, but you can&#39;t have only good months -- they&#39;d be good compared to what? So we want to make sure that the average month effect is 0, while allowing each month to be better or worse than average if needed. And the reasoning is the same for house effects for instance -- can you see why? To implement that, we use a Normal distribution whose last axis is constrained to sum to zero. In PyMC, we can use the ZeroSumNormal distribution, that Adrian Seyboldt contributed and kindly shared with us. . | Ok, enough talking, let&#39;s code! . from typing import * def ZeroSumNormal( name: str, sigma: float = 1.0, *, dims: Union[str, Tuple[str]], model: Optional[pm.Model] = None, ): &quot;&quot;&quot; Multivariate normal, such that sum(x, axis=-1) = 0. Parameters name: str String name representation of the PyMC variable. sigma: float, defaults to 1 Scale for the Normal distribution. If none is provided, a standard Normal is used. dims: Union[str, Tuple[str]] Dimension names for the shape of the distribution. See https://docs.pymc.io/pymc-examples/examples/pymc3_howto/data_container.html for an example. model: Optional[pm.Model], defaults to None PyMC model instance. If ``None``, a model instance is created. Notes - Contributed by Adrian Seyboldt (@aseyboldt). &quot;&quot;&quot; if isinstance(dims, str): dims = (dims,) model = pm.modelcontext(model) *dims_pre, dim = dims dim_trunc = f&quot;{dim}_truncated_&quot; (shape,) = model.shape_from_dims((dim,)) assert shape &gt;= 1 model.add_coords({f&quot;{dim}_truncated_&quot;: pd.RangeIndex(shape - 1)}) raw = pm.Normal( f&quot;{name}_truncated_&quot;, dims=tuple(dims_pre) + (dim_trunc,), sigma=sigma ) Q = make_sum_zero_hh(shape) draws = aet.dot(raw, Q[:, 1:].T) return pm.Deterministic(name, draws, dims=dims) def make_sum_zero_hh(N: int) -&gt; np.ndarray: &quot;&quot;&quot; Build a householder transformation matrix that maps e_1 to a vector of all 1s. &quot;&quot;&quot; e_1 = np.zeros(N) e_1[0] = 1 a = np.ones(N) a /= np.sqrt(a @ a) v = a + e_1 v /= np.sqrt(v @ v) return np.eye(N) - 2 * np.outer(v, v) . with pm.Model(coords=COORDS) as hierarchical_popularity: baseline = pm.Normal(&quot;baseline&quot;) president_effect = ZeroSumNormal(&quot;president_effect&quot;, sigma=0.15, dims=&quot;president&quot;) house_effect = ZeroSumNormal(&quot;house_effect&quot;, sigma=0.15, dims=&quot;pollster_by_method&quot;) month_effect = ZeroSumNormal(&quot;month_effect&quot;, sigma=0.15, dims=&quot;month&quot;) # need the cumsum parametrization to properly control the init of the GRW rw_init = aet.zeros(shape=(len(COORDS[&quot;president&quot;]), 1)) rw_innovations = pm.Normal( &quot;rw_innovations&quot;, dims=(&quot;president&quot;, &quot;month_minus_origin&quot;), ) raw_rw = aet.cumsum(aet.concatenate([rw_init, rw_innovations], axis=-1), axis=-1) sd = pm.HalfNormal(&quot;shrinkage_pop&quot;, 0.2) month_president_effect = pm.Deterministic( &quot;month_president_effect&quot;, raw_rw * sd, dims=(&quot;president&quot;, &quot;month&quot;) ) popularity = pm.math.invlogit( baseline + president_effect[president_id] + month_effect[month_id] + month_president_effect[president_id, month_id] + house_effect[pollster_by_method_id] ) # overdispersion parameter theta = pm.Exponential(&quot;theta_offset&quot;, 1.0) + 10.0 N_approve = pm.BetaBinomial( &quot;N_approve&quot;, alpha=popularity * theta, beta=(1.0 - popularity) * theta, n=data[&quot;samplesize&quot;], observed=data[&quot;num_approve&quot;], dims=&quot;observation&quot;, ) pm.model_to_graphviz(hierarchical_popularity) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster3 3 cluster4 4 cluster14 14 cluster15 15 cluster59 59 cluster60 60 cluster4 x 59 4 x 59 cluster4 x 60 4 x 60 cluster1,083 1,083 baseline baseline ~ Normal N_approve N_approve ~ BetaBinomial baseline&#45;&gt;N_approve theta_offset theta_offset ~ Exponential theta_offset&#45;&gt;N_approve shrinkage_pop shrinkage_pop ~ HalfNormal month_president_effect month_president_effect ~ Deterministic shrinkage_pop&#45;&gt;month_president_effect president_effect_truncated_ president_effect_truncated_ ~ Normal president_effect president_effect ~ Deterministic president_effect_truncated_&#45;&gt;president_effect president_effect&#45;&gt;N_approve house_effect_truncated_ house_effect_truncated_ ~ Normal house_effect house_effect ~ Deterministic house_effect_truncated_&#45;&gt;house_effect house_effect&#45;&gt;N_approve month_effect_truncated_ month_effect_truncated_ ~ Normal month_effect month_effect ~ Deterministic month_effect_truncated_&#45;&gt;month_effect month_effect&#45;&gt;N_approve rw_innovations rw_innovations ~ Normal rw_innovations&#45;&gt;month_president_effect month_president_effect&#45;&gt;N_approve Pssst, wanna see something funny? Let&#39;s plot the graphical representation of the very first model we tried in this study, to realize how far we&#39;ve gotten since then: . pm.model_to_graphviz(pooled_popularity_simple) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster15 15 cluster60 60 cluster1,083 1,083 house_effect house_effect ~ Normal N_approve N_approve ~ Binomial house_effect&#45;&gt;N_approve month_effect month_effect ~ GaussianRandomWalk month_effect&#45;&gt;N_approve Well we don&#39;t know about you, but we find that funny üòÇ And it&#39;s a great example of how statistical modeling happens in real life: small, incremental, error-filled steps, instead of big, giant, perfect steps -- so, in a nutshell, a delightfully miserable endeavor! . Now, let&#39;s sample from this last model! . with hierarchical_popularity: idata = pm.sample(return_inferencedata=True) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [theta_offset, shrinkage_pop, rw_innovations, month_effect_truncated_, house_effect_truncated_, president_effect_truncated_, baseline] . . 100.00% [8000/8000 02:57&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 196 seconds. . 0, dim: observation, 1083 =? 1083 . Sampling was lightning fast, with a 4x improvement over our previous model! And we don&#39;t have any warnings, aka the best of both worlds. . arviz.plot_trace( idata, var_names=[&quot;~truncated&quot;, &quot;~rw_innovations&quot;], filter_vars=&quot;regex&quot;, compact=True, ); . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: divide by zero encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/density_utils.py:783: RuntimeWarning: invalid value encountered in true_divide pdf /= bw * (2 * np.pi) ** 0.5 . arviz.summary( idata, round_to=2, var_names=[&quot;~truncated&quot;, &quot;~rw_innovations&quot;], filter_vars=&quot;regex&quot;, ) . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/diagnostics.py:561: RuntimeWarning: invalid value encountered in double_scalars (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples) /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/diagnostics.py:561: RuntimeWarning: invalid value encountered in double_scalars (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples) /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/diagnostics.py:561: RuntimeWarning: invalid value encountered in double_scalars (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples) /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/diagnostics.py:561: RuntimeWarning: invalid value encountered in double_scalars (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . baseline 0.16 | 0.08 | -0.00 | 0.31 | 0.0 | 0.00 | 1875.39 | 2361.99 | 1.01 | . president_effect[0] -0.22 | 0.06 | -0.32 | -0.11 | 0.0 | 0.00 | 5477.76 | 2964.37 | 1.00 | . president_effect[1] 0.24 | 0.06 | 0.13 | 0.35 | 0.0 | 0.00 | 5901.82 | 3281.21 | 1.00 | . president_effect[2] -0.02 | 0.06 | -0.13 | 0.08 | 0.0 | 0.00 | 6010.84 | 3175.73 | 1.00 | . president_effect[3] -0.01 | 0.05 | -0.10 | 0.08 | 0.0 | 0.00 | 4991.87 | 3454.38 | 1.00 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . month_president_effect[3,56] -0.59 | 0.39 | -1.27 | 0.18 | 0.0 | 0.00 | 6903.42 | 2799.11 | 1.00 | . month_president_effect[3,57] -0.60 | 0.40 | -1.36 | 0.16 | 0.0 | 0.00 | 7176.01 | 2852.99 | 1.00 | . month_president_effect[3,58] -0.60 | 0.42 | -1.40 | 0.21 | 0.0 | 0.00 | 7949.23 | 2993.55 | 1.00 | . month_president_effect[3,59] -0.60 | 0.44 | -1.41 | 0.22 | 0.0 | 0.00 | 8050.80 | 3096.09 | 1.00 | . theta_offset 143.78 | 7.89 | 128.92 | 158.73 | 0.1 | 0.07 | 5746.64 | 2687.64 | 1.00 | . 322 rows √ó 9 columns . Posterior predictions . And now let&#39;s do something new! Let&#39;s visualize the posterior estimates of the house effects. We&#39;ll plot the mean value for each (pollster, method) pair. Remember, a positive house effect means the given pair tend to overestimate the latent popularity: . mean_house_effect = ( idata.posterior[&quot;house_effect&quot;].mean((&quot;chain&quot;, &quot;draw&quot;)).to_dataframe() ) ax = mean_house_effect.plot.bar(figsize=(14, 7), rot=30) ax.set_xlabel(&quot;(pollster, method)&quot;) ax.set_ylabel(&quot;house effect&quot;) ax.set_title(&quot;$&gt;0$ bias means (pollster, method) overestimates the latent popularity&quot;); . All this is inline with what I usually observe when I collect the polls each month (yes, by hand, thanks for asking, that&#39;s so cute!): . BVA tends to be a bit higher than average, no matter the method. Harris tends to be higher too, while Viavoice, Elabe and, especially, YouGov tend to be report much lower results than the average pollster. | As suspected, Kantar is lower than average when using face-to-face, but is now within the average since it shifted to internet in January 2021. Interestingly, it goes the other way around for Ipsos: internet has a slightly negative bias for them, while phone has a slightly positive one. | . Now let&#39;s look at our posterior predictions. This time, we can distinguish each president, which probably helped the model tremendously: . obs_mean = data.groupby([&quot;president&quot;, &quot;month_id&quot;]).last()[&quot;p_approve_mean&quot;].unstack().T fig, axes = plt.subplots(2, 2, figsize=(14, 8), sharex=True, sharey=True) for ax, p in zip(axes.ravel(), idata.posterior.coords[&quot;president&quot;]): post = idata.posterior.sel(president=p) post_pop = logistic( ( post[&quot;baseline&quot;] + post[&quot;president_effect&quot;] + post[&quot;month_effect&quot;] + post[&quot;month_president_effect&quot;] ).stack(sample=(&quot;chain&quot;, &quot;draw&quot;)) ) # plot random posterior draws ax.plot( post.coords[&quot;month&quot;], post_pop.isel( sample=np.random.choice(post_pop.coords[&quot;sample&quot;].size, size=1000) ), alpha=0.01, color=&quot;grey&quot;, ) # plot posterior mean post_pop.mean(&quot;sample&quot;).plot(ax=ax, color=&quot;black&quot;, lw=2, label=&quot;predicted mean&quot;) # plot monthly raw polls ax.plot( obs_mean.index, obs_mean[p.data], &quot;o&quot;, color=&quot;orange&quot;, alpha=0.8, label=&quot;observed monthly&quot;, ) ax.set_xlabel(&quot;Months into term&quot;) ax.set_ylabel(&quot;Latent popularity&quot;) ax.legend() . Quite the improvement uh? The model is much, much better at tracking each president&#39;s popoularity now -- this extension to a hierarchical structure proved very necessary! . Another way to check our model&#39;s performance is to generate plausible polls from it, and compare them to the actual polls. This is a genuine posterior retrodictive check: we generate data from our model and check how plausible they are, compared to the observed data and our domain knowledge. Contrary to our previous plot, this kind of checks integrate all the model uncertainty down to the likelihood, so it&#39;s directly comparable to the observed data. . In particular, we can see in the plot above that the model still has one weakness: it has troubles when the popularity rate varies widely from one month to the next. These wild bumps happen for various reasons, usually in answer to big political events. Although they vary in magnitude, we do see a few of them in each mandate, and each time the model wasn&#39;t aggressive in enough in keeping in line with them. That could be trouble for out-of-sample predictions and could be improved in a subsequent version of the model. . Compution posterior predictive samples is just one line of code in PyMC3. We&#39;ll also extend our current InferenceData object with these posterior predictive samples, to be able to use all the xarray goodies in our plot (for a quick start on ArviZ&#39;s InferenceData&#39;s awesomness for multidimensional data, click here). . with hierarchical_popularity: idata.extend( arviz.from_pymc3( posterior_predictive=pm.sample_posterior_predictive(idata), ) ) . . 100.00% [4000/4000 00:06&lt;00:00] predicted_approval_rates = ( idata.posterior_predictive.mean((&quot;chain&quot;, &quot;draw&quot;))[&quot;N_approve&quot;] / data[&quot;samplesize&quot;] ) dates = predicted_approval_rates.field_date fig, ax = plt.subplots(figsize=(12, 4)) ax.plot( dates, data[&quot;p_approve&quot;].values, &quot;o&quot;, color=&quot;k&quot;, alpha=0.3, label=&quot;observed polls&quot; ) ax.plot(dates, predicted_approval_rates, &quot;o&quot;, alpha=0.5, label=&quot;predicted polls&quot;) ax.set_ylim(0.1, 0.8) ax.set_title(&quot;Posterior Predictive Approval Rate&quot;) ax.legend() for date in newterm_dates: ax.axvline(date, color=&quot;k&quot;, alpha=0.6, linestyle=&quot;--&quot;) . These are really good predictions üò≤ ! The model has very little trouble tracking the evolution and variation of each president&#39;s popularity, so we can be happy with ourselves. Interestingly though, we still see this tendency of the model to slightly underestimate the variation in raw polls, especially when big, sudden shifts in opinion happen, as we already mentioned. Although we don&#39;t want to exactly replicate the observed data (some polls really are outliers and that&#39;s good that the model doesn&#39;t overfit), it would be interesting to see if the model can be further improved in this respect. . And that, ladies and gentlemen, was our workflow for modeling the evolution of üá´üá∑ presidents&#39; popularity as a Markov chain! We hope you enjoyed it, and feel free to comment below or reach out for any comments or suggestions. By the way, what do you think of this model? Are you surprised that French people tend to dislike their presidents? . . Last updated: Sun May 16 2021 Python implementation: CPython Python version : 3.9.2 IPython version : 7.22.0 pandas : 1.2.4 theano : 1.1.2 numpy : 1.20.2 matplotlib: 3.4.1 pymc3 : 3.11.2 arviz : 0.11.2 .",
            "url": "https://alexandorra.github.io/pollsposition_blog/popularity/macron/hidden%20markov%20models/polls/2021/05/16/hmm-popularity.html",
            "relUrl": "/popularity/macron/hidden%20markov%20models/polls/2021/05/16/hmm-popularity.html",
            "date": " ‚Ä¢ May 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How popular is the President?",
            "content": ". Note: This is a blog post detailing how the GP model of popularity is built. To access the interactive dashboard of the model&#8217;s predictions, click here. . I like working on time series, because they usually relate to something concrete. I&#39;ve also long been intrigued by Gaussian Processes -- they have a mathematical beauty and versatility that I&#39;ve always found intriguing, if only because you can parametrize the model in ways where you can interpret it. . But... they are hard to fit -- the number of gradient computation scales with the cube of the number of data points. And in the Bayesian framework, we&#39;re trying to estimate the whole distribution of outcomes, not only one single point, which adds to the challenge. . One thing I learned so far in my open-source programming journey is not to be afraid of what you&#39;re afraid of -- to what you&#39;ll legitimately answer: &quot;wait, what??&quot;. Let me rephrase: if a method scares you, the best way to understand it is to work on an example where you need it. This will dissipate (part of) the magic behind it and help you cross a threshold in your understanding. . So that&#39;s what I did with Gaussian Processes! That all came from a simple question: how does the popularity of French presidents evolve within term and across terms? I often hear people frenetically commenting the latest popularity poll (frequently the same people who later will complain that &quot;polls are always wrong&quot;, but that&#39;s another story), and in these cases I&#39;m always worried that we&#39;re reacting to noise -- maybe it&#39;s just natural that a president experiences a dip in popularity at the middle of his term? . To answer this question, I compiled all the popularity opinion polls of French presidents since the term limits switched to 5 years (in 2002). Let&#39;s see what the data look like, before diving into the Gaussian Process model. . Show me the data! . Here are the packages we&#39;ll need: . import arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc3 as pm import xarray as xr from scipy.special import expit as logistic . Now, let&#39;s load up the data in a dataframe called d. You&#39;ll notice that, in addition to the polling data, the dataframe also contains the quarterly unemployment rate in France (downloaded from the French statistical office). As this variable is usually well correlated with politicians&#39; and parties&#39; popularity, we will use it as a predictor in our model. . As I&#39;ll explain below, we&#39;re computing the popularity every month, but since unemployment data are released quarterly, we just forward-fill the unemployment values when they are missing -- which is, I insist, an assumption, and as such it should be tested and played with, to check its impact on the model&#39;s inferences (there is a Binder and Google Collab link at the top of the page, so feel free to do so!). I could also use more intricate techniques to forecast unemployment, but that would be an overkill for our purpose here. . level_0 president samplesize p_approve p_disapprove party election_flag N_approve N_disapprove N_total unemployment . month . 2002-05-31 2002Q2 | chirac2 | 964.250000 | 0.502500 | 0.442500 | right | 1 | 485 | 427 | 912 | 7.5 | . 2002-06-30 2002Q2 | chirac2 | 970.000000 | 0.505000 | 0.425000 | right | 0 | 490 | 412 | 902 | 7.5 | . 2002-07-31 2002Q3 | chirac2 | 947.333333 | 0.533333 | 0.406667 | right | 0 | 505 | 385 | 890 | 7.5 | . 2002-08-31 2002Q3 | chirac2 | 1028.000000 | 0.520000 | 0.416667 | right | 0 | 535 | 428 | 963 | 7.5 | . 2002-09-30 2002Q3 | chirac2 | 1017.500000 | 0.525000 | 0.420000 | right | 0 | 534 | 427 | 961 | 7.5 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2020-12-31 2020Q4 | macron | 1156.500000 | 0.388333 | 0.590000 | center | 0 | 449 | 682 | 1131 | 7.7 | . 2021-01-31 2021Q1 | macron | 1184.181818 | 0.380000 | 0.589091 | center | 0 | 450 | 698 | 1148 | 7.7 | . 2021-02-28 2021Q1 | macron | 1128.625000 | 0.405000 | 0.576250 | center | 0 | 457 | 650 | 1107 | 7.7 | . 2021-03-31 2021Q1 | macron | 1100.545455 | 0.375455 | 0.582727 | center | 0 | 413 | 641 | 1054 | 7.7 | . 2021-04-30 2021Q2 | macron | 1001.666667 | 0.350000 | 0.596667 | center | 0 | 351 | 598 | 949 | 7.7 | . 228 rows √ó 11 columns . If you go check the raw underlying notebook, you&#39;ll see that there is a bunch of data cleaning involved to get to that format. The most important is that we perform a simple monthly average to get fixed time intervals, which makes computation easier for the GP -- and it&#39;s actually not that far-fetched to consider that we get to &quot;observe&quot; the president&#39;s popularity only once a month, thanks to the average of all polls taken in this month. . Ideally though, we wouldn&#39;t do that, as 1) it breaks the purely generative aspect of the model (now the model doesn&#39;t take as observations the raw polls but their average), and 2) it tricks the model into believing that the dispersion in polls is lower than it actually is. . As a first implementation though, let&#39;s make our lives easier and see how that goes -- we can always go back to the model and relax this assumption if needed. . Speaking of making our lives easier, let&#39;s write a helper function to convert datetimes to numbers, in reference to a given date. This will be useful to use time as a predictor in our model -- more precisely, as an input to our GP (completely lost? Don&#39;t worry, we&#39;ll get back to that). . def dates_to_idx(timelist): &quot;&quot;&quot;Convert datetimes to numbers in reference to a given date&quot;&quot;&quot; reference_time = timelist[0] t = (timelist - reference_time) / np.timedelta64(1, &quot;M&quot;) return np.asarray(t) time = dates_to_idx(d.index) time[:10] . array([0. , 0.98564652, 2.00414793, 3.02264934, 4.00829586, 5.02679726, 6.01244379, 7.03094519, 8.0494466 , 8.96938335]) . Let&#39;s also define a function to standardize data (mean 0 and standard deviation 1), which we&#39;ll use for our only continuous predictor, the unemployment rate. Indeed that will make it easier to set our priors for the coefficient associated to unemployment, and our sampler will have a better time sampling -- so, you know two üê¶ with one üíé ! . def standardize(series): &quot;&quot;&quot;Standardize a pandas series&quot;&quot;&quot; return (series - series.mean()) / series.std() . Build me a model . Now is time to define the model, which should make things clearer. The polls are, quite simply, realizations of a Binomial distribution: for each poll, a number $n$ of people are surveyed, and $y$ of them say they approve of the president&#39;s job. Note that we ignore those who report no opinion, because their numbers are usually negligeable, so $n - y$ represents the number of people who disapprove of the president&#39;s job. Statistically speaking, we have $y sim Binomial(n, p)$, where $p$ equals the proportion of people supporting the president. . $p$ is really what we&#39;re after here: given the observed polls, what is our inference of the true, latent support of the president in the population? But I can feel that something is bothering you, isn&#39;t it (yeah, I can read minds)? Right now you&#39;re thinking: &quot;but aren&#39;t polls noisy observations of reality&quot;? To what I&#39;ll answer: &quot;boy, you&#39;re really good at this!&quot;. Polls are indeed noisy estimates and a variety of factors account for these polling errors. . So, we need to take that into account, and a quick way to do that is to use the Beta-Binomial distribution, which handles overdispersed data -- i.e observations that are more variable than a classic Binomial distribution would expect and can accomodate. If you want more details about these models (poetically named &quot;continuous mixture models&quot;), I&#39;d refer you to chapter 12 of Richard McElreath&#39;s excellent Statistical Rethinking. . In the Beta-Binomial distribution, the Binomial probabilities are no longer fixed, but are rather random variables drawn from a common Beta distribution. So, in addition to the number of trials, $n$, Beta-Binomial distributions are parametrized by two strictly positive reals, $ alpha$ and $ beta$, which control the Beta distribution. . The thing is that $ alpha$ and $ beta$ usually don&#39;t have a real meaning, so it&#39;s often easier to parametrize the Beta distribution with two other parameters, $p$ and $ theta$, which roughly correspond to the mean and precision respectively: for a given $p$, a higher $ theta$ means that we are more skeptical of very weak or very strong probabilities -- those near 0 or 1. . We don&#39;t need a prior on $p$, as it will be a deterministic function of our regression. But we do need one for $ theta$. A value of 2 translates in a uniform prior over probabilities, which is not what we want -- we know presidential approval never goes below 10% and above 90% (at least in France). To get that, we can set $ theta = 10$. You can play around with the code below to get a sense of how the Beta family behaves: . x_plot = np.linspace(0, 1, 100) pbar = 0.5 theta = 10.0 plt.plot( x_plot, np.exp(pm.Beta.dist(pbar * theta, (1 - pbar) * theta).logp(x_plot).eval()), label=f&quot;Beta({pbar * theta}, {(1 - pbar) * theta})&quot;, ) plt.xlabel(&quot;Probablity&quot;) plt.ylabel(&quot;Density&quot;) plt.legend(); . Looks good right? And you can see that the mathematical link between $(p, theta)$ and $( alpha, beta)$ is quite simple: . $$ alpha = p times theta $$ $$ beta = (1 - p) times theta $$ . So, we want to assume that the precision is at least 10. To that end, we can use a trick and define $ theta = tilde{ theta} + 10$, where $ tilde{ theta} sim Exponential(1)$, which works because exponential distributions have a minimum of zero. . Let&#39;s turn our attention to $p$ now, the parameter we really care about. We model it through the addition of a linear component ($baseline + beta_{honeymoon} times election _ flag + beta_{unemp} times unemp _ data$ and a non-parametric component (the GP, $f _time$). The GP basically allows us to tell the model that time and popularity covary, but we don&#39;t know the exact functional form of this romantic relationship, so we&#39;d like the model to figure it out for us -- yep, GPs are pretty cool; I bet they were popular in college! To make sure $p$ stays between 0 and 1 (it&#39;s a probability, remember?), we use the logistic link function. . We now have all the parts to build our model! We just need to define the priors for all our unknown parameters. That what we&#39;ll do in the next section, but first, let&#39;s assemble all the building blocks, to contemplate our beautiful inference machine (yeah, I took a couple poetry classes in highschool): . $$y sim BetaBinomial( alpha=p times theta, : beta=(1 - p) times theta, : n)$$ . $$theta sim¬†Exponential(1) + 10$$ . $$p = logistic(baseline + f _ time + beta_{honeymoon} times election _ flag + beta_{unemp} times log(unemp _ data))$$ . $$baseline sim Normal(-0.7, 0.5)$$ . $$ beta_{honeymoon} sim Normal(-0.5, 0.3)$$ . $$ beta_{unemp} sim Normal(0, 0.2)$$ . $$f _ time sim GP(0, Sigma)$$ . $$ Sigma = amplitude^2 times Matern52(length _ scale)$$ . $$amplitude sim HalfNormal(1)$$ . $$length _ scale sim Gamma( alpha=5, beta=2)$$ . As I know you&#39;re very attentive, you have noticed that we use the logarithm of unemployment, not the raw unemployment rate. It&#39;s because we think that what matters for citizens when they think about unemployment is its order of magnitude, not its absolute values. . Other than that, the model should look pretty familiar to you now -- well, except for the priors on the GP and the coefficients. So, let&#39;s turn to that right now! . Time to choose... your GP prior . Priors are very important to fit GPs properly, so let&#39;s spend some time thinking about our priors for a more refined model of the popularity of the president. Note that this is a tutorial about how to code up a GP in PyMC3, not a tutorial about the theory of GPs, so we assume familiarity with the concepts. If you need a refresher about the theory, take a look at PyMC3&#39;s Mauna Loa notebook, Michael Betancourt&#39;s excellent case study and chapter 21 of Bayesian Data Analysis. . Kernel relations . We will use a fairly common Matern 5/2 kernel. It has the advantage of being less smooth than the classic exponentiated quadratic kernel, which is useful here as the GP must be able to jump from one regime to another very quickly when a new president is sworn in (you&#39;ll see that the swing in popularity is usually very wide). . We could probably do something better here by signaling these change points to our GP (after all, we know when a new president comes in, so we should tell it to our model). That way, we could use different kernels for each president. Or we could look into non-stationary kernels: the Matern kernel is stationary, meaning that the estimated covariance between data points doesn&#39;t vary depending on the period we&#39;re in -- maybe French people&#39;s assessment of their presidents varied less (i.e was more autocorrelated) in the 2000s, when there were no 24/7 TV news networks? . In short, it&#39;s easy to imagine that the way popularity varies with time (which is what our GP tries to capture) itself varies according to the president&#39;s personality and the times he lived in. The improvements above would integrate this assumption into the model. But, again, this is a first version -- let&#39;s see if, like the 2019 Nobel Chemistry prize, it&#39;s good enough. . Amplitude -- Don&#39;t go so high! . The Matern 5/2 kernel is parametrized with an amplitude and a length scale. The amplitude controls, quite surprisingly, the amplitude of the GP realized values: the bigger the amplitude, the larger the variations in the GP values. You can think of it as the $y$ axis in the plots you will see below -- the range of values that $f(x)$ can take on, $f$ being drawn from the GP. . Proportions in polls vary from 0 to 1, and the GP models only part of the variation in polls. We will also standardize our predictor (unemployment data), which means it will have a standard deviation of 1. So, in this context, $Halfnormal(1)$ should be a weakly regularizing prior. . Length scale -- Don&#39;t go so far! . I know you&#39;re quite impatient, so I already hear you asking: &quot;what about the length scale now?&quot;. Well, ok! But do you even know what that controls? The length scale can be interpreted as the degree of correlation between the GP values: the higher it is, the smoother the functions drawn from the GP, because their realized values will be highly correlated. In a way, you can think of the length scale as the maximum distance on the $x$ axis at which two points in time still share information. . In our case, we can imagine that polls taken 3 months ago still have a bit of influence on today&#39;s results, but it&#39;s probably not the case for polls from more than 6 months ago. So we need a prior that&#39;s both avoiding 0 (by definition, it never makes sense for the length scale to be exactly 0) and constraining values above about 6 months. The Gamma family of distributions is usually a helpful choice here, as it is very versatile and has support over the positive real line. Here are a few examples, to give you an idea: . x = np.linspace(0, 120, 500) priors = [ (r&quot;$ alpha$=5, $ beta$=2&quot;, pm.Gamma.dist(alpha=5, beta=2)), (r&quot;$ alpha$=2, $ beta$=0.5&quot;, pm.Gamma.dist(alpha=2, beta=0.5)), (r&quot;$ alpha$=9, $ beta$=1&quot;, pm.Gamma.dist(alpha=9, beta=1)), (r&quot;$ alpha$=20, $ beta$=1&quot;, pm.Gamma.dist(alpha=20, beta=1)), ] fig = plt.figure() for i, prior in enumerate(priors): plt.plot(x, np.exp(prior[1].logp(x).eval()), label=prior[0]) plt.xlim((-1, 40)) plt.xlabel(&quot;Months&quot;) plt.ylabel(&quot;Density&quot;) plt.title(&quot;Length scale priors&quot;) plt.legend(); . . The blue line, representing $Gamma(5, 2)$ has our favors, because most of the probability mass is between 0 and 6 months, which we deemed reasonable above. . If you&#39;re a bit lost, that&#39;s quite normal: GPs are rather meta, so it takes some time to develop intuition about them. A nice thing though is that we can relate the GP&#39;s parameters to the reality of our use-case, which makes it more interpretable. . In any case, take your time going through this, and understanding will come. This notebook about mean and covariance functions and this case study are good educational ressources to think about priors in the context of GPs. . Putting the GP together -- You&#39;re beautiful! . The best we can do now to make this more concrete is to draw some functions from our GP, prior to seeing any data. That will help us understand our model and determine if our prior choices make sense. This is pretty simple to do with PyMC3: . amplitude_trend = pm.HalfNormal.dist(1.0).random(1) ls_trend = pm.Gamma.dist(alpha=5, beta=2).random(1) cov_trend = amplitude_trend ** 2 * pm.gp.cov.Matern52(1, ls_trend) prior_timepoints = np.linspace(0, 60, 200)[:, None] K = cov_trend(prior_timepoints).eval() gp_prior_samples = np.random.multivariate_normal(mean=np.zeros(K.shape[0]), cov=K, size=20_000) . _, (left, mid, right) = plt.subplots( 1, 3, figsize=(14, 5), constrained_layout=True, sharex=True, sharey=True ) for ax, samples in zip((left, mid, right), (5, 10, 100)): ax.plot( prior_timepoints, gp_prior_samples[:samples].T, color=&quot;darkblue&quot;, alpha=0.3, ) ax.set_title(&quot;Samples from the GP prior&quot;) ax.set_xlabel(&quot;Time in months&quot;) ax.set_ylabel(&quot;Popularity evolution&quot;); . Each line is a realization of the GP prior. We indeed see the effect of our priors on both the amplitude and length scale, with most functions fluctuating between -1 and 1 (amplitude) and the auto-correlation being limited to around 6 months (length scale). . As we didn&#39;t specify a mean for our GP, PyMC by default centers it at 0, which happens to also be what we want: the GP is here to capture the residual variation of popularity once we&#39;ve taken into effect the baseline, unemployment and honeymoon effects. . The &quot;spaghetti&quot; plots above are useful to get an idea of individual functions and the correlations between their values, but it&#39;s hard to understand exactly where different quantiles end up. To get that information, &quot;ribbon&quot; plots are more interesting. Let&#39;s compute and display the median, 40-60%, 30-70%, 20-80% and 10-90% quantile intervals of the GP prior samples we already drew above: . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( . Easier to visualize the different expectations, right? Let&#39;s note though that these plots ignore the correlation between time points, so they underestimate the overall variation in the GP samples -- and indeed you can see that the $y$ axis is on a smaller scale than the spaghetti plots&#39;. . But wait, there is more... priors to choose . Enjoying it so far? Of course you are! Well good news: we still have priors to pick for the intercept, honeymoon and unemployment effects üçæ . Our regression intercept is also the mean function of our GP -- the value it reverts to when data start lacking. There, we have quite a lot of information: 50% popularity is historically high for a French president, so keeping the mean at zero is sub-optimal -- our parameter lives on the logit scale, so a prior centered at 0 means a prior centered at $logistic(0) = 0.5$ on the outcome space. . We can do better: based on our domain knowledge, we expect most presidents to have a baseline popularity between 20% and 50% -- in other words, French people rarely love their presidents but often really dislike them. $Normal(-0.7, 0.5)$ looks reasonable in that regard: it expects 95% of the probability mass to be between -1.7 and 0.3, i.e $logistic(-1.7) = 15 %$ and $logistic(0.3) = 57 %$, with a mean approval of $logistic(-0.7) = 33 %$: . Similarly, we have a lot of domain knowledge about the honeymoon effect -- the bump in popularity that a newly elected president gets when taking office. Here again, we have a lot of domain knowledge: we should not be surprised by big bumps, since it&#39;s not uncommon to see the outgoing president around 20-30% popularity and the new one around 60%. So, an effect centered around 40% and allowing for lower and larger effects seems appropriate: . For the unemployment effect though, we should expect a much milder effect. First, because socio-demographic variables usually have small effects in the literature. Second, because unemployment is not the only thing influencing voters&#39; opinion of the president: there is also, notably, partisanship, which makes movements in popularity less responsive to unemployment -- if you really don&#39;t like the president, you probably need to see a very low unemployment rate before starting to credit him. Finally, people probably don&#39;t know the exact current value of unemployment! They just have a fuzzy memory of where it stands -- also potentially influenced by their partisanship. . The beauty of the Bayesian framework is that we can integrate that uncertainty easily in our model: just consider the unemployment rate as a random variable! So, in practice, we put a probability distribution on the unemployment data: . $$u _uncert = log(unemp _ data) + u _diff$$ $$u _diff sim Normal(0, 0.1)$$ . Concretely, that means that unemployment is the result of the data we observe, plus some random noise around it, which conveys the uncertainty in people&#39;s mind. Why $ sigma = 0.1$ for $u _diff$, you ask? Well, data are standardized, so 0.1 is equivalent to 10% of the data&#39;s standard deviation. Since we&#39;re using a Normal distribution, this prior means that 95% of the time, we expect the &quot;true&quot; unemployment rate (the one that people have in mind when thinking about it) is equal to the observed rate $ pm , 0.2$, which seems reasonable when the observations are standardized. . All in all, we expect the unemployment to have a small negative effect, but we&#39;re not sure. So, let&#39;s center our prior on $0$ (i.e no expected effect) and use a weakly regularizing $ sigma$ (in log-odds space): $ beta_{unemp} sim Normal(0, 0.2)$. To see the effect of this prior, we have to plug it into the formula for our model, $popularity = logistic(baseline + f _ time + beta_{unemp} times u _uncert$ (we don&#39;t care about the honeymoon effect here, because the GP is already very flexible when it&#39;s not constrained by data, so you won&#39;t see a difference anyway). . We just have to generate fake unemployment data. Again, as we standardized the real data, simulating data between -3 and 3 is largely sufficient to cover the whole range of possible data: . unemp_effect_prior_samples = pm.Normal.dist(0.0, 0.2).random(size=20_000) fake_unemp = np.linspace(-3, 3, 200)[None, :] prior_approval = logistic( baseline_prior_samples[:, None] + gp_prior_samples + unemp_effect_prior_samples[:, None] * fake_unemp ) . Again, each line is a possible path for any president&#39;s approval, but this time it&#39;s not only the residual variation captured by the GP -- it&#39;s the total popularity of the president and its evolution with time. You can see that there are a lot of different paths -- a bit too many to my taste, if I&#39;m being honest. But this will be the case as long as we need a very flexible kernel to accomodate the boundary effects due to changes of presidents. It&#39;s ok for now, but if the model has troubles sampling or if we want to improve it, that&#39;s definitely one of the first things I&#39;d look at. . Now let&#39;s have a look at the marginal quantiles plot (aka ribbon plot): . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/arviz/stats/stats.py:456: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions warnings.warn( . This looks good, although it underestimates the total variation again. The tilt downwards at the boundaries is probably due to a combination of the distortion effect of the logistic link function, the rarity of data at the boundaries (which forces the model to revert to our baseline), and our baseline prior putting lots of probability mass below the baseline. And this is what we want actually: presidents spend more time around or below the baseline than above. The improvement we could make though is to tell the model that popularity should trend downwards for each term (i.e higher at the beginning than at the end). Again, we&#39;d probably need a periodic kernel for that. . Coding up the model -- Show time! . We now have everything to set up our model in PyMC3! As you&#39;ll see, it matches up the latex formulation we saw earlier quite well. There are just two novelties, to make our lives easier: we&#39;re using the coords and dims arguments instead of shape; that way, we&#39;ll get to select the data returned by our model with names instead of raw dimensions. Second, we&#39;re using the pm.Data container, to make the model aware of the data we passed; this will make posterior predictions easier down the line. . COORDS = {&quot;timesteps&quot;: d.index} with pm.Model(coords=COORDS) as econ_latent_gp: # intercept on logit scale baseline = pm.Normal(&quot;baseline&quot;, -0.7, 0.5) # honeymoon slope honeymoon = pm.Normal(&quot;honeymoon&quot;, -0.5, 0.3) # log unemployment slope log_unemp_effect = pm.Normal(&quot;log_unemp_effect&quot;, 0.0, 0.2) # long term trend amplitude_trend = pm.HalfNormal(&quot;amplitude_trend&quot;, 0.8) ls_trend = pm.Gamma(&quot;ls_trend&quot;, alpha=5, beta=2) cov_trend = amplitude_trend ** 2 * pm.gp.cov.Matern52(1, ls_trend) # instantiate gp gp = pm.gp.Latent(cov_func=cov_trend) # evaluate GP at time points f_time = gp.prior(&quot;f_time&quot;, X=time[:, None]) # data election_flag = pm.Data(&quot;election_flag&quot;, d.election_flag.values, dims=&quot;timesteps&quot;) stdz_log_unemployment = pm.Data( &quot;stdz_log_unemployment&quot;, standardize(np.log(d.unemployment)).values, dims=&quot;timesteps&quot;, ) # unemployment data is uncertain # sd = 0.1 says uncertainty on point expected btw 20% of data std 95% of time u_diff = pm.Normal(&quot;u_diff&quot;, mu=0.0, sigma=0.1, dims=&quot;timesteps&quot;) u_uncert = stdz_log_unemployment + u_diff # overdispersion parameter theta = pm.Exponential(&quot;theta_offset&quot;, 1.0) + 10.0 p = pm.Deterministic( &quot;p&quot;, pm.math.invlogit( baseline + f_time + honeymoon * election_flag + log_unemp_effect * u_uncert ), dims=&quot;timesteps&quot;, ) y = pm.BetaBinomial( &quot;y&quot;, alpha=p * theta, beta=(1.0 - p) * theta, n=d.N_total, observed=d.N_approve, dims=&quot;timesteps&quot;, ) . And now is the time to actually run our model ü•≥ We&#39;ll run 8 chains and draw 2000 samples in each of them. This is good practice because 1) each chain is quite long, so it should be able to eventually explore problematic regions if there are any, and 2) each chain starts its exploration at a different point in space, increasing our chances to explore all the typical set. . with econ_latent_gp: trace_econ = pm.sample( draws=2000, return_inferencedata=True, idata_kwargs={ &quot;dims&quot;: {&quot;f_time&quot;: [&quot;timesteps&quot;], &quot;f_time_rotated_&quot;: [&quot;timesteps&quot;]} }, ) . Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [theta_offset, u_diff, f_time_rotated_, ls_trend, amplitude_trend, log_unemp_effect, honeymoon, baseline] . . 100.00% [12000/12000 35:40&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 2157 seconds. . 0, dim: timesteps, 228 =? 228 . Diagnosing the model -- What&#39;s wrong with you?? . This is looking good! No warnings, no divergences and a really decent sampling time: 24,000 samples in 20 min (including tuning, which takes always longer), i.e 20 draws per second -- for a GP, this is really good! . So, we have no red flags here. This doesn&#39;t mean our model is good yet, but it increases the chances that it is. Let&#39;s look at the trace plot now, to make sure sampling did go well: . az.plot_trace( trace_econ, compact=True, var_names=[&quot;~u_diff&quot;, &quot;~p&quot;, &quot;~f_time&quot;, &quot;~f_time_rotated_&quot;] ); . az.plot_trace( trace_econ, var_names=[&quot;u_diff&quot;, &quot;p&quot;, &quot;f_time&quot;], compact=True, coords={&quot;timesteps&quot;: trace_econ.observed_data.timesteps[:110]}, ); . Well indeed, that looks good! The chains are mixing well for all parameters, as indicated by the KDEs (leftmost plot), and they show a random walk behavior, as evidenced by the &quot;caterpillars&quot; on the rightmost plot. . Tip: Want more details about trace plots? This blog post is really good üëå . Now let&#39;s look at the pair plot, to see if our posterior distributions exhibit high correlations or, worse, nasty funnel degeneracies üò± . az.plot_pair( trace_econ, var_names=[&quot;~u_diff&quot;, &quot;~p&quot;, &quot;~f_time&quot;, &quot;~f_time_rotated_&quot;], divergences=True, ); . Again, this is all looking good! Almost all plots look like fuzzy balls (so, don&#39;t show this plot to your cat!), hinting that there are no particular posterior correlations between our parameters. We see though that there could be a problem for high values of amplitude_trend and ls_trend (the two parameters of our GP kernel), for a reason that we would have to investigate. But since we&#39;ve thought hard about those priors beforehand and happened to choose distributions skeptical of high values, it seems that we were able to prevent those degeneracies from happening -- Minority Report style ü§ô . Everything seems to be going well here, but just for fun, let&#39;s use one of ArviZ&#39;s latest features and display the rank plot! . az.plot_rank(trace_econ, var_names=[&quot;~u_diff&quot;, &quot;~p&quot;, &quot;~f_time&quot;, &quot;~f_time_rotated_&quot;]); . &quot;That looks cool Alex... but how do I read that?&quot; ü§î Well, as stated in the original paper: . Rank plots are histograms of the ranked posterior draws (ranked over all chains) plotted separately for each chain. If all of the chains are targeting the same posterior, we expect the ranks in each chain to be uniform, whereas if one chain has a different location or scale parameter, this will be reflected in the deviation from uniformity. If rank plots of all chains look similar, this indicates good mixing of the chains. . And here, our rank plots look pretty uniform, so, again, we&#39;re all good! By the way, if you want an example of rank plots indicating problems, look at the examples on this page üòâ . Predicting popularity -- I see the future! . Plots and diagnostics are good to show us if there was a computational issue in our model, but they don&#39;t tell us if our model is good at predicting whatever it&#39;s supposed to predict. That part is always very specific to the model and use-case at hand, so there is no universal method. We could look at the summary table of our coefficients&#39; posterior: . az.summary( trace_econ, var_names=[&quot;~u_diff&quot;, &quot;~p&quot;, &quot;~f_time&quot;, &quot;~f_time_rotated_&quot;], round_to=2 ) . mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat . baseline -0.54 | 0.11 | -0.75 | -0.33 | 0.00 | 0.00 | 5069.93 | 4597.28 | 1.0 | . honeymoon 0.37 | 0.14 | 0.12 | 0.63 | 0.00 | 0.00 | 12074.93 | 5840.39 | 1.0 | . log_unemp_effect -0.11 | 0.07 | -0.23 | 0.02 | 0.00 | 0.00 | 5899.44 | 5286.19 | 1.0 | . amplitude_trend 0.43 | 0.06 | 0.32 | 0.56 | 0.00 | 0.00 | 2604.24 | 3248.72 | 1.0 | . ls_trend 6.25 | 1.01 | 4.43 | 8.14 | 0.02 | 0.01 | 2835.64 | 4307.11 | 1.0 | . theta_offset 51.70 | 6.70 | 39.10 | 64.32 | 0.07 | 0.05 | 8635.55 | 5894.67 | 1.0 | . But then you&#39;d say: &quot;uh, yeah... what does that mean? What&#39;s the relationship with popularity?&quot;. And you&#39;d be right: apart from giving us an idea of the direction and magnitude of the covariance between each parameter (all the other held constant) and the president&#39;s popularity, a simple summary table doesn&#39;t convey much. . We can use it as a &quot;gut check&quot; though: the mean baseline popularity across time is $logistic(-0.54) = 0.37$. The honeymoon effect is indeed inferred to be positive, while an increase in unemployment is associated with a slight decrease in popularity. All these inferences are consistent with our domain knowledge -- no üö© . The best way to make sense of all this though is to predict on new data (i.e &quot;out-of-sample&quot;) and visualize these predictions. So let&#39;s predict three months out-of-sample! For this, we need the time data: . MAX_OBSERVED = len(d.index) OOS_MONTHS = 3 # 1 quarter out-of-sample MAX_TIME = MAX_OBSERVED + OOS_MONTHS tnew = np.linspace(0, MAX_TIME, MAX_TIME)[:, None] . There is no presidential election on the forecasted horizon, so our new election_flag vector is all 0, which means it disappears from our regression. However, we need new unemployment data for the next three months -- and that&#39;s a bit more fun, because we get to be creative! . What are reasonable values? There are plenty of paths we could try. Conveniently, we&#39;re only forecasting one quarter, so we only need one data point for unemployment -- and then we&#39;ll add the random noise we inferred with our model (u_diff), so in reality we&#39;ll get 3 values that are close to each other. . The path we&#39;ll take here is very simple: let&#39;s assume that unemployment stays around its last value, with some noise equal to the standard deviation we observed in the sample. Let&#39;s write a small function to do all these steps easily and robustly: . log_unemp = np.log(d.unemployment) def generate_oos_unemp_data(center: float, oos_months: int) -&gt; np.ndarray: &quot;&quot;&quot; Generate out-of-sample unemployment data around `center` and for `oos_months`. &quot;&quot;&quot; # unemployment is around `center` ppc_unemp = np.random.normal( loc=center, scale=d.unemployment.std(), size=oos_months // 3, ) # data only observed quarterly, so need to forward-fill ppc_unemp = np.repeat(ppc_unemp, repeats=3) # log data and scale stdz_log_ppc_unemp = (np.log(ppc_unemp) - log_unemp.mean()) / log_unemp.std() # add noise around values return stdz_log_ppc_unemp + np.random.normal( loc=trace_econ.posterior[&quot;u_diff&quot;].mean(), scale=trace_econ.posterior[&quot;u_diff&quot;].std(), size=OOS_MONTHS, ) . oos_unemp = generate_oos_unemp_data( center=d.unemployment.iloc[-1], oos_months=OOS_MONTHS ) oos_unemp . array([ 0.11100788, 0.14303371, -0.113232 ]) . Great, now let&#39;s generate predictions, shall we? This is pretty simple, we just have to give to PyMC3 our new $unemp _ data$, $election _ flag$ and time data. Then it will automatically sample 1,000 possible paths that the president&#39;s popularity could take in the coming 3 months, depending on the future values of unemployment. In other words, we will get 1,000 different timeseries that are all compatible with our prior knowledge and the data we got to observe -- Bayesian inference baby üòé . PREDICTION_COORDS = { &quot;timesteps&quot;: pd.date_range( start=COORDS[&quot;timesteps&quot;][0], end=COORDS[&quot;timesteps&quot;][-1] + pd.DateOffset(months=OOS_MONTHS) + pd.tseries.offsets.MonthEnd(0), freq=&quot;M&quot;, ) } with econ_latent_gp: pm.set_data( { &quot;election_flag&quot;: np.concatenate( ( d.election_flag.values, np.zeros(OOS_MONTHS, dtype=int), ) ), &quot;stdz_log_unemployment&quot;: np.concatenate( (standardize(log_unemp).values, oos_unemp) ), } ) f_time_new = gp.conditional(&quot;f_time_new&quot;, Xnew=tnew) ppc = pm.sample_posterior_predictive( trace_econ.posterior, samples=1000, var_names=[&quot;baseline&quot;, &quot;f_time_new&quot;, &quot;honeymoon&quot;, &quot;log_unemp_effect&quot;], ) az.from_pymc3_predictions( ppc, idata_orig=trace_econ, inplace=True, coords=PREDICTION_COORDS, dims={&quot;f_time_new&quot;: [&quot;timesteps&quot;]}, ) . /Users/alex_andorra/opt/anaconda3/envs/elections-models/lib/python3.9/site-packages/pymc3/sampling.py:1689: UserWarning: samples parameter is smaller than nchains times ndraws, some draws and/or chains may not be represented in the returned posterior predictive sample warnings.warn( . . 100.00% [1000/1000 00:13&lt;00:00] And now we just plug these posterior predictive samples into our regression formula, to get back the predictive share of people who will approve of the president&#39;s job in the coming 3 months: . def compute_post_pred_approval( unemp_oos: xr.DataArray = trace_econ.predictions_constant_data[ &quot;stdz_log_unemployment&quot; ], ) -&gt; xr.DataArray: &quot;&quot;&quot; Compute job approval from posterior predictive samples. unemp_oos: out-of-sample values for unemployment. Defaults to the values stored in the `InferenceData.predictions_constant_data` group, i.e unemployment stays around last value. &quot;&quot;&quot; return logistic( trace_econ.predictions[&quot;baseline&quot;] + trace_econ.predictions[&quot;f_time_new&quot;] + trace_econ.predictions[&quot;honeymoon&quot;] * trace_econ.predictions_constant_data[&quot;election_flag&quot;] + trace_econ.predictions[&quot;log_unemp_effect&quot;] * unemp_oos ) . pp_prop = compute_post_pred_approval() . Great, we have computed posterior predictions and now have everything we need to plot them and see if they make sense. But first, what we can do for fun is use the same method as above to simulate posterior predictions for the cases when unemployment jumps to 10% or drops to 5% in the following quarter, instead of staying more or less the same. . Of course, this is not very realistic, as unemployment rarely changes that drastically, but this is to show-case how easy it is to generate counterfactuals in the Bayesian framework. It&#39;ll also allow us to further evaluate our model: if the different scenarios and their relative differences are consistent with our domain knowledge, this is another good point for our model. . oos_unemp_5 = generate_oos_unemp_data(center=5.0, oos_months=OOS_MONTHS) pp_prop_5 = compute_post_pred_approval( unemp_oos=xr.DataArray( np.concatenate((standardize(log_unemp).values, oos_unemp_5)), dims=[&quot;timesteps&quot;], coords=PREDICTION_COORDS, ) ) # unemployment jumps to 10% oos_unemp_10 = generate_oos_unemp_data(center=10.0, oos_months=OOS_MONTHS) pp_prop_10 = compute_post_pred_approval( unemp_oos=xr.DataArray( np.concatenate((standardize(log_unemp).values, oos_unemp_10)), dims=[&quot;timesteps&quot;], coords=PREDICTION_COORDS, ) ) . Ok, I can sense the suspense is at its peak now, so let&#39;s finally plot these predictions! The code is quite long, so I hid it by default, but the curious among you can expand it if they so wish. . _, (top, mid, low) = plt.subplots( 3, 1, figsize=(14, 12), constrained_layout=True, sharey=True ) pp_dates = pd.date_range( start=d.index[0] - np.timedelta64(0, &quot;M&quot;), periods=MAX_TIME, freq=&quot;M&quot;, ).values # plot the samples from the gp posterior with samples and shading pm.gp.util.plot_gp_dist( top, pp_prop.stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).data.T, pp_dates, plot_samples=True, palette=&quot;viridis&quot;, fill_alpha=0.3, ) pm.gp.util.plot_gp_dist( mid, pp_prop_5.stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).data.T, pp_dates, plot_samples=True, palette=&quot;cividis&quot;, fill_alpha=0.3, ) pm.gp.util.plot_gp_dist( low, pp_prop_10.stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).data.T, pp_dates, plot_samples=True, palette=&quot;inferno&quot;, fill_alpha=0.3, ) for ax, cmap, title in zip( (top, mid, low), (plt.cm.viridis(0), plt.cm.cividis(0), plt.cm.inferno(0)), (f&quot;stays at {d.unemployment.iloc[-1]}%&quot;, &quot;drops to 5%&quot;, &quot;increases to 10%&quot;), ): # plot data ax.plot( raw_polls.index, raw_polls.p_approve.values, &quot;o&quot;, ms=4, color=cmap, alpha=0.3, label=&quot;Observed polls&quot;, ) # plot historical baseline ax.hlines( logistic(trace_econ.predictions[&quot;baseline&quot;].mean()), pp_dates[0], pp_dates[-1], &quot;k&quot;, &quot;-.&quot;, alpha=0.4, lw=2, ) ax.fill_between( pp_dates, logistic(az.hdi(trace_econ.predictions)[&quot;baseline&quot;]).sel(hdi=&quot;lower&quot;), logistic(az.hdi(trace_econ.predictions)[&quot;baseline&quot;]).sel(hdi=&quot;higher&quot;), color=&quot;k&quot;, edgecolor=&quot;none&quot;, alpha=0.1, ) ax.text( pd.to_datetime(&quot;2003-09-14&quot;), 0.3, &quot;Historical approval mean&quot;, fontsize=11, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) # annotate graph ax.hlines( 0.5, pp_dates[0], pp_dates[-1], &quot;k&quot;, &quot;:&quot;, alpha=0.4, ) ax.vlines(pd.to_datetime(&quot;2002-05-14&quot;), 0.0, 0.9, &quot;k&quot;, &quot;--&quot;, alpha=0.4) ax.text( pd.to_datetime(&quot;2002-05-14&quot;), 0.9, &quot;Switch to 5-year term&quot;, fontsize=10, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) ax.vlines(pd.to_datetime(&quot;2007-05-16&quot;), 0.0, 0.9, &quot;k&quot;, &quot;--&quot;, alpha=0.4) ax.text( pd.to_datetime(&quot;2007-05-16&quot;), 0.9, &quot;Sarkozy elected&quot;, fontsize=10, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) ax.vlines(pd.to_datetime(&quot;2012-05-11&quot;), 0.0, 0.9, &quot;k&quot;, &quot;--&quot;, alpha=0.4) ax.text( pd.to_datetime(&quot;2012-05-11&quot;), 0.9, &quot;Hollande elected&quot;, fontsize=10, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) ax.vlines(pd.to_datetime(&quot;2017-05-17&quot;), 0.0, 0.9, &quot;k&quot;, &quot;--&quot;, alpha=0.4) ax.text( pd.to_datetime(&quot;2017-05-17&quot;), 0.92, &quot;Macron elected&quot;, fontsize=10, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) ax.vlines(pd.to_datetime(&quot;2020-01-24&quot;), 0.0, 0.9, &quot;k&quot;, &quot;--&quot;, alpha=0.4) ax.text( pd.to_datetime(&quot;2020-03-17&quot;), 0.91, &quot;1st Covid Cases&quot;, fontsize=10, color=&quot;grey&quot;, horizontalalignment=&quot;center&quot;, ) # axis labels and title ax.set_ylim((0, 1)) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;popularity in %&quot;) ax.set_title( f&quot;1 quarter out-of-sample, if unemployment {title}&quot;, fontsize=14, fontstyle=&quot;italic&quot;, ) ax.legend(fontsize=11, loc=&quot;lower left&quot;, frameon=True) plt.suptitle( &quot;Evolution of French presidents&#39; popularity over time&quot;, fontsize=14, fontweight=&quot;bold&quot;, ) plt.savefig(&quot;../../pollsposition_models/popularity/gp-popularity&quot;) . . Well I&#39;d say this is looking pretty good! As advertised, for each of the three counterfactuals, we got a whole distribution of time series -- 1,000 of them to be precise, each time series spanning from 2002 to the current month + one quarter. . This article was a tutorial to show you the model, so, for the lessons we can draw from this plot, let&#39;s go to the interactive version of this plot üòâ . . I hope you enjoyed this tutorial and that now you fancy trying out PyMC3 in your own domain -- if you also learned a thing or two about how French presidents&#39; popularity evolves with time, well that&#39;s even better! If you have any questions, suggestions, or spot a mistake somewhere (yeah, I&#39;m a pretty great man but I do make mistakes too -- 1.5 per year on average, to be precise), feel free to comment below or open an issue on the GitHub repo. . For anything else (especially immoderate praise), you can reach out on Twitter, and if you&#39;d like to learn more about Bayesian inference, feel free to check out my &quot;Learning Bayesian Statistics&quot; podcast, where I interview practitioners and researchers about why and how they use these methods. . Thanks for reading! Keep calm and PyMCheers üññ . PS: I wanted to leave you on a poetic note, so here is a list of the great packages I used to run this analysis üòç . Author: AlexAndorra Last updated: Fri Apr 30 2021 Python implementation: CPython Python version : 3.9.2 IPython version : 7.22.0 xarray : 0.17.0 numpy : 1.20.2 pymc3 : 3.11.2 arviz : 0.11.2 pandas : 1.2.4 matplotlib: 3.4.1 .",
            "url": "https://alexandorra.github.io/pollsposition_blog/popularity/macron/gaussian%20processes/polls/2021/01/18/gp-popularity.html",
            "relUrl": "/popularity/macron/gaussian%20processes/polls/2021/01/18/gp-popularity.html",
            "date": " ‚Ä¢ Jan 18, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "The PollsPosition project is an open-source endeavor with three main goals: . Exploring new statistical methods and applying them to real-life issues and datasets. This is a way for me to gain new skills and stay up to date with the field of Bayesian statistics, where the fun part is that you&#39;re never done learning! As such, I like to call PollsPosition my &quot;nerdy sandbox&quot; ü§ì Understanding how and why people make decisions when they don‚Äôt have all the facts is fascinating to me. That‚Äôs why I like, among others, electoral forecasting, and I use it as a sparring partner. | Popularizing and illustrating the power of Bayesian statistics. These are especially helpful in contexts where data are sparse and imprecise, domain knowledge is important, and estimating uncertainty is one of the main interest of the analysis. Well look at that: democratic elections check all these boxes ‚úÖ Icing on the cake, French political parties are numerous and change in nature -- compared to the US for instance -- which makes the models all the more complicated, but also... interesting! If you&#39;re curious, I presented a talk in 2020 about a version of the model: | A last, broader goal is to try to counteract our natural tendencies (especially during electoral campaigns!) to cherry-pick data, overreact to the latest poll, and, most importantly, completely misinterpret uncertainties and probabilities. I was invited on a podcast in 2020 to talk about just that, if that&#39;s of interest to you üìª I&#39;m perfectly aware that it&#39;s a lofty and probably unattainable goal. I do believe that spreading the methods of rational and critical thinking is essential though, so I can at least try ü§∑‚Äç‚ôÇ For sure, I can&#39;t do it alone, so if you like what we do at PollsPosition, feel free to share our content with your friends and colleagues -- or if you don&#39;t like it, share it with people you don&#39;t like, that works too! The Local Maximum ¬∑ Ep. 140 - Why Polls are Tricky with Alex Andorra | If this all sounds fun to you and you&#39;re looking for a project to improve your Python and Bayesian chops, feel free to contribute pull requests -- there is always something to do! . My name is Alexandre Andorra by the way. By day, I&#39;m a Bayesian modeler at the PyMC Labs consultancy and host the most popular podcast dedicated to Bayesian inference out there -- aka Learning Bayesian Statistics . By night, I don&#39;t (yet) fight crime, but I&#39;m an open-source enthusiast and core contributor to the awesome Python packages PyMC and ArviZ . . An always-learning statistician, I love building models and studying elections and human behavior. I also love Nutella a bit too much, but I don&#39;t like talking about it ‚Äì I prefer eating it üòã . I can&#39;t finish without acknowledging the people who help me in this nerdy adventure, most notably the brilliant Alexis Berg√®s who devises with me for hours about ways to best model elections, as well as the wonderful core-developers of ArviZ and PyMC, who often indulge my unrelenting stats questions ü§© . Feel free to reach out on Twitter if you want to talk about chocolate, statistical modeling under certainty, or how &quot;polls are useless now because they missed two elections in a row!&quot; -- yeah, I&#39;m a bit sarcastic. .",
          "url": "https://alexandorra.github.io/pollsposition_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://alexandorra.github.io/pollsposition_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}